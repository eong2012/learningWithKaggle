{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/home/ubuntu/data/iceberg'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Conv2D, Cropping2D\n",
    "from keras.layers import MaxPooling2D, ZeroPadding2D, BatchNormalization, Activation\n",
    "from keras.layers.merge import Add, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import cv2\n",
    "import keras\n",
    "import os\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras import __version__\n",
    "print(__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "\n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "#         imgs.append(np.dstack((band_1, band_2, band_3)))\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)\n",
    "\n",
    "def get_more_images(imgs):\n",
    "    \n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        a=imgs[i,:,:,0]\n",
    "        b=imgs[i,:,:,1]\n",
    "        c=imgs[i,:,:,2]\n",
    "        \n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        bv=cv2.flip(b,1)\n",
    "        bh=cv2.flip(b,0)\n",
    "        cv=cv2.flip(c,1)\n",
    "        ch=cv2.flip(c,0)\n",
    "        \n",
    "        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n",
    "        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h))\n",
    "    \n",
    "    return more_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional image features from David\n",
    "train_features = pd.read_csv(\"/home/ubuntu/data/iceberg/features/train.csv\", index_col=0)\n",
    "train_features.drop(labels=[\"is_iceberg\", \"inc_angle\"], inplace=True, axis=1)\n",
    "#test_features = pd.read_csv(\"iceberg_features/test.csv\", index_col=0)\n",
    "\n",
    "num_features = train_features.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_json(os.path.join(data_dir, 'train.json'))\n",
    "\n",
    "df_join = pd.merge(df_train, train_features, on='id', how=\"outer\")\n",
    "\n",
    "Xtrain = get_scaled_imgs(df_join)\n",
    "Ytrain = np.array(df_join['is_iceberg'])\n",
    "\n",
    "df_join.inc_angle = df_join.inc_angle.replace('na',0)\n",
    "idx_tr = np.where(df_join.inc_angle>0)\n",
    "\n",
    "Ytrain = Ytrain[idx_tr[0]]\n",
    "Xtrain = Xtrain[idx_tr[0],...]\n",
    "Xfeatures = df_join.drop(labels=[\"id\", \"is_iceberg\", \"band_1\", \"band_2\"], axis=1).iloc[idx_tr[0]]\n",
    "\n",
    "inc_angle = Xfeatures.iloc[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc_angle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "Xfeatures.iloc[:,1:179] = min_max_scaler.fit_transform(Xfeatures.iloc[:,1:179])\n",
    "Xfeatures = pd.DataFrame(Xfeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inc_angle</th>\n",
       "      <th>band1.0.m.cx</th>\n",
       "      <th>band1.0.m.cy</th>\n",
       "      <th>band1.0.m.majoraxis</th>\n",
       "      <th>band1.0.m.eccentricity</th>\n",
       "      <th>band1.0.m.theta</th>\n",
       "      <th>band1.0.s.area</th>\n",
       "      <th>band1.0.s.perimeter</th>\n",
       "      <th>band1.0.s.radius.mean</th>\n",
       "      <th>band1.0.s.radius.sd</th>\n",
       "      <th>...</th>\n",
       "      <th>band2.Ba.h.var.s2</th>\n",
       "      <th>band2.Ba.h.idm.s2</th>\n",
       "      <th>band2.Ba.h.sav.s2</th>\n",
       "      <th>band2.Ba.h.sva.s2</th>\n",
       "      <th>band2.Ba.h.sen.s2</th>\n",
       "      <th>band2.Ba.h.ent.s2</th>\n",
       "      <th>band2.Ba.h.dva.s2</th>\n",
       "      <th>band2.Ba.h.den.s2</th>\n",
       "      <th>band2.Ba.h.f12.s2</th>\n",
       "      <th>band2.Ba.h.f13.s2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.9239</td>\n",
       "      <td>0.494756</td>\n",
       "      <td>0.495616</td>\n",
       "      <td>0.103221</td>\n",
       "      <td>0.941760</td>\n",
       "      <td>0.042102</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.085889</td>\n",
       "      <td>0.112781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602761</td>\n",
       "      <td>0.737116</td>\n",
       "      <td>0.821546</td>\n",
       "      <td>0.745714</td>\n",
       "      <td>0.379207</td>\n",
       "      <td>0.377427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126811</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.116523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.1562</td>\n",
       "      <td>0.485197</td>\n",
       "      <td>0.532935</td>\n",
       "      <td>0.103426</td>\n",
       "      <td>0.951659</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.009470</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.079175</td>\n",
       "      <td>0.118316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.2859</td>\n",
       "      <td>0.511777</td>\n",
       "      <td>0.497717</td>\n",
       "      <td>0.044357</td>\n",
       "      <td>0.789014</td>\n",
       "      <td>0.089221</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.043723</td>\n",
       "      <td>0.046857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852684</td>\n",
       "      <td>0.538941</td>\n",
       "      <td>0.329327</td>\n",
       "      <td>0.196049</td>\n",
       "      <td>0.451034</td>\n",
       "      <td>0.489892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187851</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.082524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.8306</td>\n",
       "      <td>0.562976</td>\n",
       "      <td>0.470446</td>\n",
       "      <td>0.034102</td>\n",
       "      <td>0.707574</td>\n",
       "      <td>0.105458</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.034812</td>\n",
       "      <td>0.041439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.6256</td>\n",
       "      <td>0.500823</td>\n",
       "      <td>0.450152</td>\n",
       "      <td>0.046437</td>\n",
       "      <td>0.687752</td>\n",
       "      <td>0.085616</td>\n",
       "      <td>0.006807</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>0.054164</td>\n",
       "      <td>0.036080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36.9034</td>\n",
       "      <td>0.468283</td>\n",
       "      <td>0.437232</td>\n",
       "      <td>0.138158</td>\n",
       "      <td>0.793867</td>\n",
       "      <td>0.293086</td>\n",
       "      <td>0.032554</td>\n",
       "      <td>0.029224</td>\n",
       "      <td>0.167948</td>\n",
       "      <td>0.100572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856995</td>\n",
       "      <td>0.596586</td>\n",
       "      <td>0.679501</td>\n",
       "      <td>0.570047</td>\n",
       "      <td>0.626598</td>\n",
       "      <td>0.615360</td>\n",
       "      <td>0.068169</td>\n",
       "      <td>0.322802</td>\n",
       "      <td>0.029238</td>\n",
       "      <td>0.208841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.4751</td>\n",
       "      <td>0.500316</td>\n",
       "      <td>0.469714</td>\n",
       "      <td>0.197512</td>\n",
       "      <td>0.960440</td>\n",
       "      <td>0.180585</td>\n",
       "      <td>0.026931</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.178466</td>\n",
       "      <td>0.221128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673442</td>\n",
       "      <td>0.545668</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.166950</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.817931</td>\n",
       "      <td>0.254438</td>\n",
       "      <td>0.706719</td>\n",
       "      <td>0.082374</td>\n",
       "      <td>0.403971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41.1769</td>\n",
       "      <td>0.483258</td>\n",
       "      <td>0.475220</td>\n",
       "      <td>0.170729</td>\n",
       "      <td>0.876345</td>\n",
       "      <td>0.911024</td>\n",
       "      <td>0.035809</td>\n",
       "      <td>0.037443</td>\n",
       "      <td>0.189673</td>\n",
       "      <td>0.158450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35.7829</td>\n",
       "      <td>0.608955</td>\n",
       "      <td>0.533824</td>\n",
       "      <td>0.130403</td>\n",
       "      <td>0.941166</td>\n",
       "      <td>0.970553</td>\n",
       "      <td>0.015981</td>\n",
       "      <td>0.023744</td>\n",
       "      <td>0.114970</td>\n",
       "      <td>0.138804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301488</td>\n",
       "      <td>0.727982</td>\n",
       "      <td>0.900095</td>\n",
       "      <td>0.829867</td>\n",
       "      <td>0.420391</td>\n",
       "      <td>0.413174</td>\n",
       "      <td>0.141144</td>\n",
       "      <td>0.334383</td>\n",
       "      <td>0.040803</td>\n",
       "      <td>0.202811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43.3007</td>\n",
       "      <td>0.507456</td>\n",
       "      <td>0.499572</td>\n",
       "      <td>0.023223</td>\n",
       "      <td>0.632850</td>\n",
       "      <td>0.864680</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>0.025690</td>\n",
       "      <td>0.022076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44.6240</td>\n",
       "      <td>0.507881</td>\n",
       "      <td>0.489986</td>\n",
       "      <td>0.132239</td>\n",
       "      <td>0.959426</td>\n",
       "      <td>0.069143</td>\n",
       "      <td>0.013909</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.105591</td>\n",
       "      <td>0.145997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803288</td>\n",
       "      <td>0.593016</td>\n",
       "      <td>0.730903</td>\n",
       "      <td>0.618026</td>\n",
       "      <td>0.445170</td>\n",
       "      <td>0.470245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174745</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.015114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>39.5067</td>\n",
       "      <td>0.703127</td>\n",
       "      <td>0.538464</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>0.615838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.011802</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>41.8544</td>\n",
       "      <td>0.432626</td>\n",
       "      <td>0.483828</td>\n",
       "      <td>0.252503</td>\n",
       "      <td>0.908809</td>\n",
       "      <td>0.987247</td>\n",
       "      <td>0.058597</td>\n",
       "      <td>0.055708</td>\n",
       "      <td>0.265307</td>\n",
       "      <td>0.245502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941678</td>\n",
       "      <td>0.642322</td>\n",
       "      <td>0.559552</td>\n",
       "      <td>0.456244</td>\n",
       "      <td>0.730774</td>\n",
       "      <td>0.686338</td>\n",
       "      <td>0.074667</td>\n",
       "      <td>0.385036</td>\n",
       "      <td>0.117414</td>\n",
       "      <td>0.444057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>45.2909</td>\n",
       "      <td>0.495175</td>\n",
       "      <td>0.494708</td>\n",
       "      <td>0.084088</td>\n",
       "      <td>0.768833</td>\n",
       "      <td>0.822158</td>\n",
       "      <td>0.014797</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.099392</td>\n",
       "      <td>0.064962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207826</td>\n",
       "      <td>0.892004</td>\n",
       "      <td>0.083615</td>\n",
       "      <td>0.029230</td>\n",
       "      <td>0.169588</td>\n",
       "      <td>0.166149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057507</td>\n",
       "      <td>0.014697</td>\n",
       "      <td>0.076985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34.7715</td>\n",
       "      <td>0.580172</td>\n",
       "      <td>0.510137</td>\n",
       "      <td>0.066883</td>\n",
       "      <td>0.825186</td>\n",
       "      <td>0.036998</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.012785</td>\n",
       "      <td>0.069563</td>\n",
       "      <td>0.061077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926589</td>\n",
       "      <td>0.424405</td>\n",
       "      <td>0.589543</td>\n",
       "      <td>0.440325</td>\n",
       "      <td>0.639311</td>\n",
       "      <td>0.673072</td>\n",
       "      <td>0.073375</td>\n",
       "      <td>0.386910</td>\n",
       "      <td>0.028314</td>\n",
       "      <td>0.214821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>43.7820</td>\n",
       "      <td>0.564903</td>\n",
       "      <td>0.496349</td>\n",
       "      <td>0.053471</td>\n",
       "      <td>0.764604</td>\n",
       "      <td>0.128440</td>\n",
       "      <td>0.007103</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.057696</td>\n",
       "      <td>0.049258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529303</td>\n",
       "      <td>0.687825</td>\n",
       "      <td>0.848633</td>\n",
       "      <td>0.763449</td>\n",
       "      <td>0.307493</td>\n",
       "      <td>0.333390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145174</td>\n",
       "      <td>0.067150</td>\n",
       "      <td>0.234917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>45.3568</td>\n",
       "      <td>0.510889</td>\n",
       "      <td>0.498328</td>\n",
       "      <td>0.086547</td>\n",
       "      <td>0.910485</td>\n",
       "      <td>0.049937</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015525</td>\n",
       "      <td>0.074656</td>\n",
       "      <td>0.095545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>38.7812</td>\n",
       "      <td>0.664435</td>\n",
       "      <td>0.522134</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.204806</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.023238</td>\n",
       "      <td>0.019419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315026</td>\n",
       "      <td>0.501247</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.728413</td>\n",
       "      <td>0.343169</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.390630</td>\n",
       "      <td>0.151066</td>\n",
       "      <td>0.396555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>42.5145</td>\n",
       "      <td>0.516467</td>\n",
       "      <td>0.488601</td>\n",
       "      <td>0.073179</td>\n",
       "      <td>0.885540</td>\n",
       "      <td>0.037888</td>\n",
       "      <td>0.008286</td>\n",
       "      <td>0.012785</td>\n",
       "      <td>0.065009</td>\n",
       "      <td>0.078342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.896718</td>\n",
       "      <td>0.390693</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.436082</td>\n",
       "      <td>0.706216</td>\n",
       "      <td>0.736286</td>\n",
       "      <td>0.293816</td>\n",
       "      <td>0.465779</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>0.212428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37.2802</td>\n",
       "      <td>0.508908</td>\n",
       "      <td>0.506966</td>\n",
       "      <td>0.165570</td>\n",
       "      <td>0.977585</td>\n",
       "      <td>0.053075</td>\n",
       "      <td>0.013613</td>\n",
       "      <td>0.026484</td>\n",
       "      <td>0.120132</td>\n",
       "      <td>0.186334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805941</td>\n",
       "      <td>0.307650</td>\n",
       "      <td>0.557745</td>\n",
       "      <td>0.390692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995771</td>\n",
       "      <td>0.402045</td>\n",
       "      <td>0.707201</td>\n",
       "      <td>0.077665</td>\n",
       "      <td>0.431088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>41.7973</td>\n",
       "      <td>0.502601</td>\n",
       "      <td>0.512698</td>\n",
       "      <td>0.054303</td>\n",
       "      <td>0.870147</td>\n",
       "      <td>0.152090</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.047783</td>\n",
       "      <td>0.063331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>38.0669</td>\n",
       "      <td>0.528681</td>\n",
       "      <td>0.490548</td>\n",
       "      <td>0.117456</td>\n",
       "      <td>0.869121</td>\n",
       "      <td>0.965237</td>\n",
       "      <td>0.019236</td>\n",
       "      <td>0.022831</td>\n",
       "      <td>0.124378</td>\n",
       "      <td>0.116948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798601</td>\n",
       "      <td>0.527637</td>\n",
       "      <td>0.346628</td>\n",
       "      <td>0.219848</td>\n",
       "      <td>0.671626</td>\n",
       "      <td>0.670569</td>\n",
       "      <td>0.161654</td>\n",
       "      <td>0.436895</td>\n",
       "      <td>0.026931</td>\n",
       "      <td>0.209106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>39.6636</td>\n",
       "      <td>0.525046</td>\n",
       "      <td>0.490162</td>\n",
       "      <td>0.256009</td>\n",
       "      <td>0.959219</td>\n",
       "      <td>0.456612</td>\n",
       "      <td>0.045280</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.223558</td>\n",
       "      <td>0.258312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491928</td>\n",
       "      <td>0.724650</td>\n",
       "      <td>0.855435</td>\n",
       "      <td>0.780878</td>\n",
       "      <td>0.450942</td>\n",
       "      <td>0.439259</td>\n",
       "      <td>0.052045</td>\n",
       "      <td>0.263101</td>\n",
       "      <td>0.014130</td>\n",
       "      <td>0.122592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>37.6866</td>\n",
       "      <td>0.563276</td>\n",
       "      <td>0.502511</td>\n",
       "      <td>0.144139</td>\n",
       "      <td>0.892920</td>\n",
       "      <td>0.435669</td>\n",
       "      <td>0.023084</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.137404</td>\n",
       "      <td>0.152407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978821</td>\n",
       "      <td>0.479708</td>\n",
       "      <td>0.586263</td>\n",
       "      <td>0.443211</td>\n",
       "      <td>0.496011</td>\n",
       "      <td>0.542405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198670</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.063064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40.2960</td>\n",
       "      <td>0.579566</td>\n",
       "      <td>0.493473</td>\n",
       "      <td>0.254767</td>\n",
       "      <td>0.936391</td>\n",
       "      <td>0.718697</td>\n",
       "      <td>0.052678</td>\n",
       "      <td>0.044749</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>0.247623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697057</td>\n",
       "      <td>0.669519</td>\n",
       "      <td>0.781955</td>\n",
       "      <td>0.687618</td>\n",
       "      <td>0.464304</td>\n",
       "      <td>0.464501</td>\n",
       "      <td>0.080559</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0.018813</td>\n",
       "      <td>0.145535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>39.2340</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.458891</td>\n",
       "      <td>0.337747</td>\n",
       "      <td>0.782898</td>\n",
       "      <td>0.447206</td>\n",
       "      <td>0.106540</td>\n",
       "      <td>0.141553</td>\n",
       "      <td>0.363951</td>\n",
       "      <td>0.346284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310710</td>\n",
       "      <td>0.854499</td>\n",
       "      <td>0.115954</td>\n",
       "      <td>0.054136</td>\n",
       "      <td>0.289592</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.049173</td>\n",
       "      <td>0.145322</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.134276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40.3904</td>\n",
       "      <td>0.155359</td>\n",
       "      <td>0.086455</td>\n",
       "      <td>0.485858</td>\n",
       "      <td>0.861855</td>\n",
       "      <td>0.335115</td>\n",
       "      <td>0.147677</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>0.454849</td>\n",
       "      <td>0.488436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914729</td>\n",
       "      <td>0.540954</td>\n",
       "      <td>0.613118</td>\n",
       "      <td>0.487839</td>\n",
       "      <td>0.770699</td>\n",
       "      <td>0.752811</td>\n",
       "      <td>0.096199</td>\n",
       "      <td>0.465267</td>\n",
       "      <td>0.039416</td>\n",
       "      <td>0.268015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>43.7895</td>\n",
       "      <td>0.724556</td>\n",
       "      <td>0.550728</td>\n",
       "      <td>0.018771</td>\n",
       "      <td>0.526520</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>0.021913</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>42.5891</td>\n",
       "      <td>0.501128</td>\n",
       "      <td>0.487532</td>\n",
       "      <td>0.128751</td>\n",
       "      <td>0.939541</td>\n",
       "      <td>0.066580</td>\n",
       "      <td>0.015685</td>\n",
       "      <td>0.022831</td>\n",
       "      <td>0.113474</td>\n",
       "      <td>0.137718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950822</td>\n",
       "      <td>0.445022</td>\n",
       "      <td>0.623264</td>\n",
       "      <td>0.473262</td>\n",
       "      <td>0.463842</td>\n",
       "      <td>0.526124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203115</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.166496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41.0303</td>\n",
       "      <td>0.690230</td>\n",
       "      <td>0.505875</td>\n",
       "      <td>0.059853</td>\n",
       "      <td>0.832407</td>\n",
       "      <td>0.121812</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.061070</td>\n",
       "      <td>0.056878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711261</td>\n",
       "      <td>0.692628</td>\n",
       "      <td>0.776442</td>\n",
       "      <td>0.688250</td>\n",
       "      <td>0.425152</td>\n",
       "      <td>0.426647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143470</td>\n",
       "      <td>0.015534</td>\n",
       "      <td>0.126710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>31.0446</td>\n",
       "      <td>0.469263</td>\n",
       "      <td>0.459613</td>\n",
       "      <td>0.211512</td>\n",
       "      <td>0.785762</td>\n",
       "      <td>0.026083</td>\n",
       "      <td>0.062740</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.247977</td>\n",
       "      <td>0.178525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>0.596026</td>\n",
       "      <td>0.396691</td>\n",
       "      <td>0.285772</td>\n",
       "      <td>0.780095</td>\n",
       "      <td>0.739060</td>\n",
       "      <td>0.094450</td>\n",
       "      <td>0.474294</td>\n",
       "      <td>0.089644</td>\n",
       "      <td>0.401449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>42.4133</td>\n",
       "      <td>0.563381</td>\n",
       "      <td>0.495022</td>\n",
       "      <td>0.067249</td>\n",
       "      <td>0.837857</td>\n",
       "      <td>0.223180</td>\n",
       "      <td>0.008582</td>\n",
       "      <td>0.011872</td>\n",
       "      <td>0.068417</td>\n",
       "      <td>0.064847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>44.6188</td>\n",
       "      <td>0.525300</td>\n",
       "      <td>0.507718</td>\n",
       "      <td>0.374199</td>\n",
       "      <td>0.779673</td>\n",
       "      <td>0.054982</td>\n",
       "      <td>0.188813</td>\n",
       "      <td>0.118721</td>\n",
       "      <td>0.476719</td>\n",
       "      <td>0.306135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785090</td>\n",
       "      <td>0.706547</td>\n",
       "      <td>0.298157</td>\n",
       "      <td>0.204988</td>\n",
       "      <td>0.535859</td>\n",
       "      <td>0.511833</td>\n",
       "      <td>0.021725</td>\n",
       "      <td>0.222097</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.295083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>34.9664</td>\n",
       "      <td>0.583122</td>\n",
       "      <td>0.515671</td>\n",
       "      <td>0.382935</td>\n",
       "      <td>0.997399</td>\n",
       "      <td>0.950177</td>\n",
       "      <td>0.036993</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.324871</td>\n",
       "      <td>0.464416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500344</td>\n",
       "      <td>0.791883</td>\n",
       "      <td>0.172526</td>\n",
       "      <td>0.096229</td>\n",
       "      <td>0.334172</td>\n",
       "      <td>0.326101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104228</td>\n",
       "      <td>0.027551</td>\n",
       "      <td>0.147881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>32.4245</td>\n",
       "      <td>0.561162</td>\n",
       "      <td>0.519936</td>\n",
       "      <td>0.323886</td>\n",
       "      <td>0.969629</td>\n",
       "      <td>0.476911</td>\n",
       "      <td>0.063628</td>\n",
       "      <td>0.055708</td>\n",
       "      <td>0.279334</td>\n",
       "      <td>0.328653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768642</td>\n",
       "      <td>0.650825</td>\n",
       "      <td>0.741660</td>\n",
       "      <td>0.643478</td>\n",
       "      <td>0.516128</td>\n",
       "      <td>0.511365</td>\n",
       "      <td>0.017109</td>\n",
       "      <td>0.226588</td>\n",
       "      <td>0.018498</td>\n",
       "      <td>0.151375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>31.0609</td>\n",
       "      <td>0.591148</td>\n",
       "      <td>0.550320</td>\n",
       "      <td>0.241860</td>\n",
       "      <td>0.939974</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>0.048831</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.237612</td>\n",
       "      <td>0.254602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995153</td>\n",
       "      <td>0.674757</td>\n",
       "      <td>0.481831</td>\n",
       "      <td>0.382147</td>\n",
       "      <td>0.542053</td>\n",
       "      <td>0.525554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149713</td>\n",
       "      <td>0.088149</td>\n",
       "      <td>0.337463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>39.6097</td>\n",
       "      <td>0.501929</td>\n",
       "      <td>0.488829</td>\n",
       "      <td>0.032281</td>\n",
       "      <td>0.846841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.023514</td>\n",
       "      <td>0.043632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>39.2340</td>\n",
       "      <td>0.644315</td>\n",
       "      <td>0.590204</td>\n",
       "      <td>0.056540</td>\n",
       "      <td>0.870247</td>\n",
       "      <td>0.161906</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>0.052652</td>\n",
       "      <td>0.059001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996110</td>\n",
       "      <td>0.438085</td>\n",
       "      <td>0.485352</td>\n",
       "      <td>0.330858</td>\n",
       "      <td>0.487210</td>\n",
       "      <td>0.546824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203824</td>\n",
       "      <td>0.012158</td>\n",
       "      <td>0.126803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>41.4339</td>\n",
       "      <td>0.544395</td>\n",
       "      <td>0.535235</td>\n",
       "      <td>0.139162</td>\n",
       "      <td>0.874556</td>\n",
       "      <td>0.242376</td>\n",
       "      <td>0.022788</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.139319</td>\n",
       "      <td>0.141024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946853</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.433667</td>\n",
       "      <td>0.282216</td>\n",
       "      <td>0.573762</td>\n",
       "      <td>0.612172</td>\n",
       "      <td>0.074292</td>\n",
       "      <td>0.293782</td>\n",
       "      <td>0.042261</td>\n",
       "      <td>0.250703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>40.9855</td>\n",
       "      <td>0.687005</td>\n",
       "      <td>0.736220</td>\n",
       "      <td>0.151129</td>\n",
       "      <td>0.899354</td>\n",
       "      <td>0.440001</td>\n",
       "      <td>0.026931</td>\n",
       "      <td>0.029224</td>\n",
       "      <td>0.149774</td>\n",
       "      <td>0.144529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434607</td>\n",
       "      <td>0.711491</td>\n",
       "      <td>0.869792</td>\n",
       "      <td>0.796075</td>\n",
       "      <td>0.464283</td>\n",
       "      <td>0.453182</td>\n",
       "      <td>0.039553</td>\n",
       "      <td>0.306238</td>\n",
       "      <td>0.024336</td>\n",
       "      <td>0.163619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>42.5707</td>\n",
       "      <td>0.565207</td>\n",
       "      <td>0.517578</td>\n",
       "      <td>0.181464</td>\n",
       "      <td>0.848694</td>\n",
       "      <td>0.999396</td>\n",
       "      <td>0.045280</td>\n",
       "      <td>0.039269</td>\n",
       "      <td>0.210344</td>\n",
       "      <td>0.147118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993277</td>\n",
       "      <td>0.586409</td>\n",
       "      <td>0.521596</td>\n",
       "      <td>0.403721</td>\n",
       "      <td>0.590408</td>\n",
       "      <td>0.588991</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>0.236455</td>\n",
       "      <td>0.028192</td>\n",
       "      <td>0.200653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>38.4755</td>\n",
       "      <td>0.560934</td>\n",
       "      <td>0.501050</td>\n",
       "      <td>0.027489</td>\n",
       "      <td>0.652373</td>\n",
       "      <td>0.526870</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.029062</td>\n",
       "      <td>0.027212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.334026</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.338039</td>\n",
       "      <td>0.429520</td>\n",
       "      <td>0.529185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206378</td>\n",
       "      <td>0.081704</td>\n",
       "      <td>0.325763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>45.2416</td>\n",
       "      <td>0.569950</td>\n",
       "      <td>0.473695</td>\n",
       "      <td>0.170968</td>\n",
       "      <td>0.955622</td>\n",
       "      <td>0.450914</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.139660</td>\n",
       "      <td>0.184321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174037</td>\n",
       "      <td>0.910807</td>\n",
       "      <td>0.956752</td>\n",
       "      <td>0.930550</td>\n",
       "      <td>0.148963</td>\n",
       "      <td>0.144377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048002</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.063276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>34.4718</td>\n",
       "      <td>0.531061</td>\n",
       "      <td>0.478359</td>\n",
       "      <td>0.154620</td>\n",
       "      <td>0.980331</td>\n",
       "      <td>0.238223</td>\n",
       "      <td>0.013613</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.126569</td>\n",
       "      <td>0.179447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>35.2957</td>\n",
       "      <td>0.471370</td>\n",
       "      <td>0.448730</td>\n",
       "      <td>0.275468</td>\n",
       "      <td>0.919919</td>\n",
       "      <td>0.295442</td>\n",
       "      <td>0.068067</td>\n",
       "      <td>0.059361</td>\n",
       "      <td>0.289281</td>\n",
       "      <td>0.247576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.780017</td>\n",
       "      <td>0.588216</td>\n",
       "      <td>0.308485</td>\n",
       "      <td>0.190712</td>\n",
       "      <td>0.630259</td>\n",
       "      <td>0.620688</td>\n",
       "      <td>0.088704</td>\n",
       "      <td>0.370969</td>\n",
       "      <td>0.016515</td>\n",
       "      <td>0.157465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>34.1367</td>\n",
       "      <td>0.549453</td>\n",
       "      <td>0.487993</td>\n",
       "      <td>0.163852</td>\n",
       "      <td>0.915213</td>\n",
       "      <td>0.227478</td>\n",
       "      <td>0.027819</td>\n",
       "      <td>0.031963</td>\n",
       "      <td>0.156644</td>\n",
       "      <td>0.163373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525680</td>\n",
       "      <td>0.746742</td>\n",
       "      <td>0.849912</td>\n",
       "      <td>0.777848</td>\n",
       "      <td>0.339688</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123003</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.030620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>39.6416</td>\n",
       "      <td>0.420542</td>\n",
       "      <td>0.514502</td>\n",
       "      <td>0.090483</td>\n",
       "      <td>0.782689</td>\n",
       "      <td>0.281442</td>\n",
       "      <td>0.015685</td>\n",
       "      <td>0.019178</td>\n",
       "      <td>0.101951</td>\n",
       "      <td>0.077157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>41.1402</td>\n",
       "      <td>0.506658</td>\n",
       "      <td>0.482562</td>\n",
       "      <td>0.052523</td>\n",
       "      <td>0.556445</td>\n",
       "      <td>0.516988</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.012785</td>\n",
       "      <td>0.067202</td>\n",
       "      <td>0.042328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983057</td>\n",
       "      <td>0.565669</td>\n",
       "      <td>0.578804</td>\n",
       "      <td>0.455349</td>\n",
       "      <td>0.525392</td>\n",
       "      <td>0.542285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181738</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>0.112890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>39.2166</td>\n",
       "      <td>0.475718</td>\n",
       "      <td>0.467238</td>\n",
       "      <td>0.131651</td>\n",
       "      <td>0.915882</td>\n",
       "      <td>0.223611</td>\n",
       "      <td>0.016869</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.127668</td>\n",
       "      <td>0.148294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905444</td>\n",
       "      <td>0.421844</td>\n",
       "      <td>0.646673</td>\n",
       "      <td>0.495074</td>\n",
       "      <td>0.585856</td>\n",
       "      <td>0.631128</td>\n",
       "      <td>0.141513</td>\n",
       "      <td>0.367462</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.197126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>40.3416</td>\n",
       "      <td>0.518695</td>\n",
       "      <td>0.487575</td>\n",
       "      <td>0.074506</td>\n",
       "      <td>0.874601</td>\n",
       "      <td>0.035076</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.070891</td>\n",
       "      <td>0.074504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>38.1366</td>\n",
       "      <td>0.480648</td>\n",
       "      <td>0.508883</td>\n",
       "      <td>0.074046</td>\n",
       "      <td>0.948910</td>\n",
       "      <td>0.073961</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.011872</td>\n",
       "      <td>0.051684</td>\n",
       "      <td>0.090053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>35.2954</td>\n",
       "      <td>0.498314</td>\n",
       "      <td>0.500411</td>\n",
       "      <td>0.147134</td>\n",
       "      <td>0.724587</td>\n",
       "      <td>0.530414</td>\n",
       "      <td>0.039065</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.182608</td>\n",
       "      <td>0.113967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965564</td>\n",
       "      <td>0.529627</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.337436</td>\n",
       "      <td>0.626827</td>\n",
       "      <td>0.633858</td>\n",
       "      <td>0.046446</td>\n",
       "      <td>0.305151</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>0.118452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>34.4718</td>\n",
       "      <td>0.549256</td>\n",
       "      <td>0.494823</td>\n",
       "      <td>0.158543</td>\n",
       "      <td>0.734801</td>\n",
       "      <td>0.135639</td>\n",
       "      <td>0.039361</td>\n",
       "      <td>0.040183</td>\n",
       "      <td>0.193654</td>\n",
       "      <td>0.146148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938461</td>\n",
       "      <td>0.488763</td>\n",
       "      <td>0.563542</td>\n",
       "      <td>0.431448</td>\n",
       "      <td>0.761679</td>\n",
       "      <td>0.754161</td>\n",
       "      <td>0.118056</td>\n",
       "      <td>0.471310</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>0.224458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>36.1049</td>\n",
       "      <td>0.457974</td>\n",
       "      <td>0.456842</td>\n",
       "      <td>0.156950</td>\n",
       "      <td>0.915193</td>\n",
       "      <td>0.100202</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.142181</td>\n",
       "      <td>0.176443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462462</td>\n",
       "      <td>0.775744</td>\n",
       "      <td>0.159758</td>\n",
       "      <td>0.079489</td>\n",
       "      <td>0.310374</td>\n",
       "      <td>0.311427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111112</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.023515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>44.5704</td>\n",
       "      <td>0.511777</td>\n",
       "      <td>0.497033</td>\n",
       "      <td>0.057891</td>\n",
       "      <td>0.904455</td>\n",
       "      <td>0.076443</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.068253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.889349</td>\n",
       "      <td>0.556018</td>\n",
       "      <td>0.677083</td>\n",
       "      <td>0.553893</td>\n",
       "      <td>0.477742</td>\n",
       "      <td>0.506646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>45.2814</td>\n",
       "      <td>0.501018</td>\n",
       "      <td>0.495907</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.917258</td>\n",
       "      <td>0.066769</td>\n",
       "      <td>0.008582</td>\n",
       "      <td>0.015525</td>\n",
       "      <td>0.072748</td>\n",
       "      <td>0.103670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803288</td>\n",
       "      <td>0.556018</td>\n",
       "      <td>0.300347</td>\n",
       "      <td>0.170706</td>\n",
       "      <td>0.429520</td>\n",
       "      <td>0.467882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184029</td>\n",
       "      <td>0.010250</td>\n",
       "      <td>0.107697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>35.7829</td>\n",
       "      <td>0.549705</td>\n",
       "      <td>0.472807</td>\n",
       "      <td>0.062645</td>\n",
       "      <td>0.916342</td>\n",
       "      <td>0.984728</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.047112</td>\n",
       "      <td>0.077326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>34.4721</td>\n",
       "      <td>0.485457</td>\n",
       "      <td>0.468309</td>\n",
       "      <td>0.157776</td>\n",
       "      <td>0.749268</td>\n",
       "      <td>0.272590</td>\n",
       "      <td>0.039953</td>\n",
       "      <td>0.040183</td>\n",
       "      <td>0.198552</td>\n",
       "      <td>0.160521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>37.9814</td>\n",
       "      <td>0.556957</td>\n",
       "      <td>0.484651</td>\n",
       "      <td>0.038047</td>\n",
       "      <td>0.810674</td>\n",
       "      <td>0.115967</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>0.034494</td>\n",
       "      <td>0.040591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937759</td>\n",
       "      <td>0.500520</td>\n",
       "      <td>0.636719</td>\n",
       "      <td>0.499681</td>\n",
       "      <td>0.482372</td>\n",
       "      <td>0.525694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195315</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.065527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>35.2987</td>\n",
       "      <td>0.434476</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.293573</td>\n",
       "      <td>0.964730</td>\n",
       "      <td>0.229044</td>\n",
       "      <td>0.053862</td>\n",
       "      <td>0.056621</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.337049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776848</td>\n",
       "      <td>0.639509</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>0.639246</td>\n",
       "      <td>0.483512</td>\n",
       "      <td>0.488238</td>\n",
       "      <td>0.012934</td>\n",
       "      <td>0.196817</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.136885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1471 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      inc_angle  band1.0.m.cx  band1.0.m.cy  band1.0.m.majoraxis  \\\n",
       "0       43.9239      0.494756      0.495616             0.103221   \n",
       "1       38.1562      0.485197      0.532935             0.103426   \n",
       "2       45.2859      0.511777      0.497717             0.044357   \n",
       "3       43.8306      0.562976      0.470446             0.034102   \n",
       "4       35.6256      0.500823      0.450152             0.046437   \n",
       "5       36.9034      0.468283      0.437232             0.138158   \n",
       "6       34.4751      0.500316      0.469714             0.197512   \n",
       "7       41.1769      0.483258      0.475220             0.170729   \n",
       "8       35.7829      0.608955      0.533824             0.130403   \n",
       "9       43.3007      0.507456      0.499572             0.023223   \n",
       "10      44.6240      0.507881      0.489986             0.132239   \n",
       "11      39.5067      0.703127      0.538464             0.014579   \n",
       "12      41.8544      0.432626      0.483828             0.252503   \n",
       "13      45.2909      0.495175      0.494708             0.084088   \n",
       "14      34.7715      0.580172      0.510137             0.066883   \n",
       "15      43.7820      0.564903      0.496349             0.053471   \n",
       "16      45.3568      0.510889      0.498328             0.086547   \n",
       "17      38.7812      0.664435      0.522134             0.017619   \n",
       "18      42.5145      0.516467      0.488601             0.073179   \n",
       "19      37.2802      0.508908      0.506966             0.165570   \n",
       "20      41.7973      0.502601      0.512698             0.054303   \n",
       "21      38.0669      0.528681      0.490548             0.117456   \n",
       "22      39.6636      0.525046      0.490162             0.256009   \n",
       "23      37.6866      0.563276      0.502511             0.144139   \n",
       "24      40.2960      0.579566      0.493473             0.254767   \n",
       "25      39.2340      0.440700      0.458891             0.337747   \n",
       "26      40.3904      0.155359      0.086455             0.485858   \n",
       "27      43.7895      0.724556      0.550728             0.018771   \n",
       "28      42.5891      0.501128      0.487532             0.128751   \n",
       "29      41.0303      0.690230      0.505875             0.059853   \n",
       "...         ...           ...           ...                  ...   \n",
       "1479    31.0446      0.469263      0.459613             0.211512   \n",
       "1480    42.4133      0.563381      0.495022             0.067249   \n",
       "1481    44.6188      0.525300      0.507718             0.374199   \n",
       "1482    34.9664      0.583122      0.515671             0.382935   \n",
       "1483    32.4245      0.561162      0.519936             0.323886   \n",
       "1484    31.0609      0.591148      0.550320             0.241860   \n",
       "1485    39.6097      0.501929      0.488829             0.032281   \n",
       "1486    39.2340      0.644315      0.590204             0.056540   \n",
       "1487    41.4339      0.544395      0.535235             0.139162   \n",
       "1488    40.9855      0.687005      0.736220             0.151129   \n",
       "1489    42.5707      0.565207      0.517578             0.181464   \n",
       "1490    38.4755      0.560934      0.501050             0.027489   \n",
       "1491    45.2416      0.569950      0.473695             0.170968   \n",
       "1492    34.4718      0.531061      0.478359             0.154620   \n",
       "1493    35.2957      0.471370      0.448730             0.275468   \n",
       "1494    34.1367      0.549453      0.487993             0.163852   \n",
       "1495    39.6416      0.420542      0.514502             0.090483   \n",
       "1496    41.1402      0.506658      0.482562             0.052523   \n",
       "1497    39.2166      0.475718      0.467238             0.131651   \n",
       "1498    40.3416      0.518695      0.487575             0.074506   \n",
       "1499    38.1366      0.480648      0.508883             0.074046   \n",
       "1500    35.2954      0.498314      0.500411             0.147134   \n",
       "1501    34.4718      0.549256      0.494823             0.158543   \n",
       "1502    36.1049      0.457974      0.456842             0.156950   \n",
       "1503    44.5704      0.511777      0.497033             0.057891   \n",
       "1504    45.2814      0.501018      0.495907             0.088210   \n",
       "1505    35.7829      0.549705      0.472807             0.062645   \n",
       "1506    34.4721      0.485457      0.468309             0.157776   \n",
       "1507    37.9814      0.556957      0.484651             0.038047   \n",
       "1508    35.2987      0.434476      0.432727             0.293573   \n",
       "\n",
       "      band1.0.m.eccentricity  band1.0.m.theta  band1.0.s.area  \\\n",
       "0                   0.941760         0.042102        0.010654   \n",
       "1                   0.951659         0.501300        0.009470   \n",
       "2                   0.789014         0.089221        0.005031   \n",
       "3                   0.707574         0.105458        0.003847   \n",
       "4                   0.687752         0.085616        0.006807   \n",
       "5                   0.793867         0.293086        0.032554   \n",
       "6                   0.960440         0.180585        0.026931   \n",
       "7                   0.876345         0.911024        0.035809   \n",
       "8                   0.941166         0.970553        0.015981   \n",
       "9                   0.632850         0.864680        0.002664   \n",
       "10                  0.959426         0.069143        0.013909   \n",
       "11                  0.615838         1.000000        0.001480   \n",
       "12                  0.908809         0.987247        0.058597   \n",
       "13                  0.768833         0.822158        0.014797   \n",
       "14                  0.825186         0.036998        0.009174   \n",
       "15                  0.764604         0.128440        0.007103   \n",
       "16                  0.910485         0.049937        0.009766   \n",
       "17                  0.204806         0.500000        0.002664   \n",
       "18                  0.885540         0.037888        0.008286   \n",
       "19                  0.977585         0.053075        0.013613   \n",
       "20                  0.870147         0.152090        0.005327   \n",
       "21                  0.869121         0.965237        0.019236   \n",
       "22                  0.959219         0.456612        0.045280   \n",
       "23                  0.892920         0.435669        0.023084   \n",
       "24                  0.936391         0.718697        0.052678   \n",
       "25                  0.782898         0.447206        0.106540   \n",
       "26                  0.861855         0.335115        0.147677   \n",
       "27                  0.526520         0.250000        0.002368   \n",
       "28                  0.939541         0.066580        0.015685   \n",
       "29                  0.832407         0.121812        0.007399   \n",
       "...                      ...              ...             ...   \n",
       "1479                0.785762         0.026083        0.062740   \n",
       "1480                0.837857         0.223180        0.008582   \n",
       "1481                0.779673         0.054982        0.188813   \n",
       "1482                0.997399         0.950177        0.036993   \n",
       "1483                0.969629         0.476911        0.063628   \n",
       "1484                0.939974         0.214790        0.048831   \n",
       "1485                0.846841         1.000000        0.002664   \n",
       "1486                0.870247         0.161906        0.005919   \n",
       "1487                0.874556         0.242376        0.022788   \n",
       "1488                0.899354         0.440001        0.026931   \n",
       "1489                0.848694         0.999396        0.045280   \n",
       "1490                0.652373         0.526870        0.003255   \n",
       "1491                0.955622         0.450914        0.022492   \n",
       "1492                0.980331         0.238223        0.013613   \n",
       "1493                0.919919         0.295442        0.068067   \n",
       "1494                0.915213         0.227478        0.027819   \n",
       "1495                0.782689         0.281442        0.015685   \n",
       "1496                0.556445         0.516988        0.009174   \n",
       "1497                0.915882         0.223611        0.016869   \n",
       "1498                0.874601         0.035076        0.009174   \n",
       "1499                0.948910         0.073961        0.005623   \n",
       "1500                0.724587         0.530414        0.039065   \n",
       "1501                0.734801         0.135639        0.039361   \n",
       "1502                0.915193         0.100202        0.020716   \n",
       "1503                0.904455         0.076443        0.005031   \n",
       "1504                0.917258         0.066769        0.008582   \n",
       "1505                0.916342         0.984728        0.005327   \n",
       "1506                0.749268         0.272590        0.039953   \n",
       "1507                0.810674         0.115967        0.003847   \n",
       "1508                0.964730         0.229044        0.053862   \n",
       "\n",
       "      band1.0.s.perimeter  band1.0.s.radius.mean  band1.0.s.radius.sd  \\\n",
       "0                0.018265               0.085889             0.112781   \n",
       "1                0.017352               0.079175             0.118316   \n",
       "2                0.008219               0.043723             0.046857   \n",
       "3                0.006393               0.034812             0.041439   \n",
       "4                0.010046               0.054164             0.036080   \n",
       "5                0.029224               0.167948             0.100572   \n",
       "6                0.032877               0.178466             0.221128   \n",
       "7                0.037443               0.189673             0.158450   \n",
       "8                0.023744               0.114970             0.138804   \n",
       "9                0.004566               0.025690             0.022076   \n",
       "10               0.021918               0.105591             0.145997   \n",
       "11               0.003653               0.011802             0.023289   \n",
       "12               0.055708               0.265307             0.245502   \n",
       "13               0.017352               0.099392             0.064962   \n",
       "14               0.012785               0.069563             0.061077   \n",
       "15               0.010959               0.057696             0.049258   \n",
       "16               0.015525               0.074656             0.095545   \n",
       "17               0.005479               0.023238             0.019419   \n",
       "18               0.012785               0.065009             0.078342   \n",
       "19               0.026484               0.120132             0.186334   \n",
       "20               0.009132               0.047783             0.063331   \n",
       "21               0.022831               0.124378             0.116948   \n",
       "22               0.045662               0.223558             0.258312   \n",
       "23               0.028311               0.137404             0.152407   \n",
       "24               0.044749               0.244531             0.247623   \n",
       "25               0.141553               0.363951             0.346284   \n",
       "26               0.115982               0.454849             0.488436   \n",
       "27               0.004566               0.021913             0.015388   \n",
       "28               0.022831               0.113474             0.137718   \n",
       "29               0.010959               0.061070             0.056878   \n",
       "...                   ...                    ...                  ...   \n",
       "1479             0.050228               0.247977             0.178525   \n",
       "1480             0.011872               0.068417             0.064847   \n",
       "1481             0.118721               0.476719             0.306135   \n",
       "1482             0.068493               0.324871             0.464416   \n",
       "1483             0.055708               0.279334             0.328653   \n",
       "1484             0.050228               0.237612             0.254602   \n",
       "1485             0.005479               0.023514             0.043632   \n",
       "1486             0.010046               0.052652             0.059001   \n",
       "1487             0.027397               0.139319             0.141024   \n",
       "1488             0.029224               0.149774             0.144529   \n",
       "1489             0.039269               0.210344             0.147118   \n",
       "1490             0.006393               0.029062             0.027212   \n",
       "1491             0.028311               0.139660             0.184321   \n",
       "1492             0.021918               0.126569             0.179447   \n",
       "1493             0.059361               0.289281             0.247576   \n",
       "1494             0.031963               0.156644             0.163373   \n",
       "1495             0.019178               0.101951             0.077157   \n",
       "1496             0.012785               0.067202             0.042328   \n",
       "1497             0.028311               0.127668             0.148294   \n",
       "1498             0.013699               0.070891             0.074504   \n",
       "1499             0.011872               0.051684             0.090053   \n",
       "1500             0.032877               0.182608             0.113967   \n",
       "1501             0.040183               0.193654             0.146148   \n",
       "1502             0.032877               0.142181             0.176443   \n",
       "1503             0.010959               0.045100             0.068253   \n",
       "1504             0.015525               0.072748             0.103670   \n",
       "1505             0.010959               0.047112             0.077326   \n",
       "1506             0.040183               0.198552             0.160521   \n",
       "1507             0.007306               0.034494             0.040591   \n",
       "1508             0.056621               0.273310             0.337049   \n",
       "\n",
       "            ...          band2.Ba.h.var.s2  band2.Ba.h.idm.s2  \\\n",
       "0           ...                   0.602761           0.737116   \n",
       "1           ...                   0.004145           1.000000   \n",
       "2           ...                   0.852684           0.538941   \n",
       "3           ...                   0.004145           1.000000   \n",
       "4           ...                   0.004145           1.000000   \n",
       "5           ...                   0.856995           0.596586   \n",
       "6           ...                   0.673442           0.545668   \n",
       "7           ...                   0.004145           1.000000   \n",
       "8           ...                   0.301488           0.727982   \n",
       "9           ...                   0.004145           1.000000   \n",
       "10          ...                   0.803288           0.593016   \n",
       "11          ...                   0.004145           1.000000   \n",
       "12          ...                   0.941678           0.642322   \n",
       "13          ...                   0.207826           0.892004   \n",
       "14          ...                   0.926589           0.424405   \n",
       "15          ...                   0.529303           0.687825   \n",
       "16          ...                   0.004145           1.000000   \n",
       "17          ...                   0.315026           0.501247   \n",
       "18          ...                   0.896718           0.390693   \n",
       "19          ...                   0.805941           0.307650   \n",
       "20          ...                   0.004145           1.000000   \n",
       "21          ...                   0.798601           0.527637   \n",
       "22          ...                   0.491928           0.724650   \n",
       "23          ...                   0.978821           0.479708   \n",
       "24          ...                   0.697057           0.669519   \n",
       "25          ...                   0.310710           0.854499   \n",
       "26          ...                   0.914729           0.540954   \n",
       "27          ...                   0.004145           1.000000   \n",
       "28          ...                   0.950822           0.445022   \n",
       "29          ...                   0.711261           0.692628   \n",
       "...         ...                        ...                ...   \n",
       "1479        ...                   0.881197           0.596026   \n",
       "1480        ...                   0.004145           1.000000   \n",
       "1481        ...                   0.785090           0.706547   \n",
       "1482        ...                   0.500344           0.791883   \n",
       "1483        ...                   0.768642           0.650825   \n",
       "1484        ...                   0.995153           0.674757   \n",
       "1485        ...                   0.004145           1.000000   \n",
       "1486        ...                   0.996110           0.438085   \n",
       "1487        ...                   0.946853           0.453468   \n",
       "1488        ...                   0.434607           0.711491   \n",
       "1489        ...                   0.993277           0.586409   \n",
       "1490        ...                   1.000000           0.334026   \n",
       "1491        ...                   0.174037           0.910807   \n",
       "1492        ...                   0.004145           1.000000   \n",
       "1493        ...                   0.780017           0.588216   \n",
       "1494        ...                   0.525680           0.746742   \n",
       "1495        ...                   0.004145           1.000000   \n",
       "1496        ...                   0.983057           0.565669   \n",
       "1497        ...                   0.905444           0.421844   \n",
       "1498        ...                   0.004145           1.000000   \n",
       "1499        ...                   0.004145           1.000000   \n",
       "1500        ...                   0.965564           0.529627   \n",
       "1501        ...                   0.938461           0.488763   \n",
       "1502        ...                   0.462462           0.775744   \n",
       "1503        ...                   0.889349           0.556018   \n",
       "1504        ...                   0.803288           0.556018   \n",
       "1505        ...                   0.004145           1.000000   \n",
       "1506        ...                   0.004145           1.000000   \n",
       "1507        ...                   0.937759           0.500520   \n",
       "1508        ...                   0.776848           0.639509   \n",
       "\n",
       "      band2.Ba.h.sav.s2  band2.Ba.h.sva.s2  band2.Ba.h.sen.s2  \\\n",
       "0              0.821546           0.745714           0.379207   \n",
       "1              1.000000           1.000000           0.000000   \n",
       "2              0.329327           0.196049           0.451034   \n",
       "3              1.000000           1.000000           0.000000   \n",
       "4              1.000000           1.000000           0.000000   \n",
       "5              0.679501           0.570047           0.626598   \n",
       "6              0.270833           0.166950           0.848921   \n",
       "7              0.031250           0.000977           0.000000   \n",
       "8              0.900095           0.829867           0.420391   \n",
       "9              1.000000           1.000000           0.000000   \n",
       "10             0.730903           0.618026           0.445170   \n",
       "11             1.000000           1.000000           0.000000   \n",
       "12             0.559552           0.456244           0.730774   \n",
       "13             0.083615           0.029230           0.169588   \n",
       "14             0.589543           0.440325           0.639311   \n",
       "15             0.848633           0.763449           0.307493   \n",
       "16             0.031250           0.000977           0.000000   \n",
       "17             0.843750           0.728413           0.343169   \n",
       "18             0.593750           0.436082           0.706216   \n",
       "19             0.557745           0.390692           1.000000   \n",
       "20             1.000000           1.000000           0.000000   \n",
       "21             0.346628           0.219848           0.671626   \n",
       "22             0.855435           0.780878           0.450942   \n",
       "23             0.586263           0.443211           0.496011   \n",
       "24             0.781955           0.687618           0.464304   \n",
       "25             0.115954           0.054136           0.289592   \n",
       "26             0.613118           0.487839           0.770699   \n",
       "27             1.000000           1.000000           0.000000   \n",
       "28             0.623264           0.473262           0.463842   \n",
       "29             0.776442           0.688250           0.425152   \n",
       "...                 ...                ...                ...   \n",
       "1479           0.396691           0.285772           0.780095   \n",
       "1480           1.000000           1.000000           0.000000   \n",
       "1481           0.298157           0.204988           0.535859   \n",
       "1482           0.172526           0.096229           0.334172   \n",
       "1483           0.741660           0.643478           0.516128   \n",
       "1484           0.481831           0.382147           0.542053   \n",
       "1485           1.000000           1.000000           0.000000   \n",
       "1486           0.485352           0.330858           0.487210   \n",
       "1487           0.433667           0.282216           0.573762   \n",
       "1488           0.869792           0.796075           0.464283   \n",
       "1489           0.521596           0.403721           0.590408   \n",
       "1490           0.515625           0.338039           0.429520   \n",
       "1491           0.956752           0.930550           0.148963   \n",
       "1492           1.000000           1.000000           0.000000   \n",
       "1493           0.308485           0.190712           0.630259   \n",
       "1494           0.849912           0.777848           0.339688   \n",
       "1495           1.000000           1.000000           0.000000   \n",
       "1496           0.578804           0.455349           0.525392   \n",
       "1497           0.646673           0.495074           0.585856   \n",
       "1498           0.031250           0.000977           0.000000   \n",
       "1499           0.031250           0.000977           0.000000   \n",
       "1500           0.468750           0.337436           0.626827   \n",
       "1501           0.563542           0.431448           0.761679   \n",
       "1502           0.159758           0.079489           0.310374   \n",
       "1503           0.677083           0.553893           0.477742   \n",
       "1504           0.300347           0.170706           0.429520   \n",
       "1505           1.000000           1.000000           0.000000   \n",
       "1506           0.031250           0.000977           0.000000   \n",
       "1507           0.636719           0.499681           0.482372   \n",
       "1508           0.741071           0.639246           0.483512   \n",
       "\n",
       "      band2.Ba.h.ent.s2  band2.Ba.h.dva.s2  band2.Ba.h.den.s2  \\\n",
       "0              0.377427           0.000000           0.126811   \n",
       "1              0.000000           0.000000           0.000000   \n",
       "2              0.489892           0.000000           0.187851   \n",
       "3              0.000000           0.000000           0.000000   \n",
       "4              0.000000           0.000000           0.000000   \n",
       "5              0.615360           0.068169           0.322802   \n",
       "6              0.817931           0.254438           0.706719   \n",
       "7              0.000000           0.000000           0.000000   \n",
       "8              0.413174           0.141144           0.334383   \n",
       "9              0.000000           0.000000           0.000000   \n",
       "10             0.470245           0.000000           0.174745   \n",
       "11             0.000000           0.000000           0.000000   \n",
       "12             0.686338           0.074667           0.385036   \n",
       "13             0.166149           0.000000           0.057507   \n",
       "14             0.673072           0.073375           0.386910   \n",
       "15             0.333390           0.000000           0.145174   \n",
       "16             0.000000           0.000000           0.000000   \n",
       "17             0.413793           0.595238           0.390630   \n",
       "18             0.736286           0.293816           0.465779   \n",
       "19             0.995771           0.402045           0.707201   \n",
       "20             0.000000           0.000000           0.000000   \n",
       "21             0.670569           0.161654           0.436895   \n",
       "22             0.439259           0.052045           0.263101   \n",
       "23             0.542405           0.000000           0.198670   \n",
       "24             0.464501           0.080559           0.210884   \n",
       "25             0.273000           0.049173           0.145322   \n",
       "26             0.752811           0.096199           0.465267   \n",
       "27             0.000000           0.000000           0.000000   \n",
       "28             0.526124           0.000000           0.203115   \n",
       "29             0.426647           0.000000           0.143470   \n",
       "...                 ...                ...                ...   \n",
       "1479           0.739060           0.094450           0.474294   \n",
       "1480           0.000000           0.000000           0.000000   \n",
       "1481           0.511833           0.021725           0.222097   \n",
       "1482           0.326101           0.000000           0.104228   \n",
       "1483           0.511365           0.017109           0.226588   \n",
       "1484           0.525554           0.000000           0.149713   \n",
       "1485           0.000000           0.000000           0.000000   \n",
       "1486           0.546824           0.000000           0.203824   \n",
       "1487           0.612172           0.074292           0.293782   \n",
       "1488           0.453182           0.039553           0.306238   \n",
       "1489           0.588991           0.017567           0.236455   \n",
       "1490           0.529185           0.000000           0.206378   \n",
       "1491           0.144377           0.000000           0.048002   \n",
       "1492           0.000000           0.000000           0.000000   \n",
       "1493           0.620688           0.088704           0.370969   \n",
       "1494           0.343000           0.000000           0.123003   \n",
       "1495           0.000000           0.000000           0.000000   \n",
       "1496           0.542285           0.000000           0.181738   \n",
       "1497           0.631128           0.141513           0.367462   \n",
       "1498           0.000000           0.000000           0.000000   \n",
       "1499           0.000000           0.000000           0.000000   \n",
       "1500           0.633858           0.046446           0.305151   \n",
       "1501           0.754161           0.118056           0.471310   \n",
       "1502           0.311427           0.000000           0.111112   \n",
       "1503           0.506646           0.000000           0.184029   \n",
       "1504           0.467882           0.000000           0.184029   \n",
       "1505           0.000000           0.000000           0.000000   \n",
       "1506           0.000000           0.000000           0.000000   \n",
       "1507           0.525694           0.000000           0.195315   \n",
       "1508           0.488238           0.012934           0.196817   \n",
       "\n",
       "      band2.Ba.h.f12.s2  band2.Ba.h.f13.s2  \n",
       "0              0.014846           0.116523  \n",
       "1              0.000000           0.000000  \n",
       "2              0.005754           0.082524  \n",
       "3              0.000000           0.000000  \n",
       "4              0.000000           0.000000  \n",
       "5              0.029238           0.208841  \n",
       "6              0.082374           0.403971  \n",
       "7              0.000000           0.000000  \n",
       "8              0.040803           0.202811  \n",
       "9              0.000000           0.000000  \n",
       "10             0.000201           0.015114  \n",
       "11             0.000000           0.000000  \n",
       "12             0.117414           0.444057  \n",
       "13             0.014697           0.076985  \n",
       "14             0.028314           0.214821  \n",
       "15             0.067150           0.234917  \n",
       "16             0.000000           0.000000  \n",
       "17             0.151066           0.396555  \n",
       "18             0.025342           0.212428  \n",
       "19             0.077665           0.431088  \n",
       "20             0.000000           0.000000  \n",
       "21             0.026931           0.209106  \n",
       "22             0.014130           0.122592  \n",
       "23             0.003037           0.063064  \n",
       "24             0.018813           0.145535  \n",
       "25             0.027115           0.134276  \n",
       "26             0.039416           0.268015  \n",
       "27             0.000000           0.000000  \n",
       "28             0.021739           0.166496  \n",
       "29             0.015534           0.126710  \n",
       "...                 ...                ...  \n",
       "1479           0.089644           0.401449  \n",
       "1480           0.000000           0.000000  \n",
       "1481           0.069444           0.295083  \n",
       "1482           0.027551           0.147881  \n",
       "1483           0.018498           0.151375  \n",
       "1484           0.088149           0.337463  \n",
       "1485           0.000000           0.000000  \n",
       "1486           0.012158           0.126803  \n",
       "1487           0.042261           0.250703  \n",
       "1488           0.024336           0.163619  \n",
       "1489           0.028192           0.200653  \n",
       "1490           0.081704           0.325763  \n",
       "1491           0.011439           0.063276  \n",
       "1492           0.000000           0.000000  \n",
       "1493           0.016515           0.157465  \n",
       "1494           0.001133           0.030620  \n",
       "1495           0.000000           0.000000  \n",
       "1496           0.009722           0.112890  \n",
       "1497           0.025421           0.197126  \n",
       "1498           0.000000           0.000000  \n",
       "1499           0.000000           0.000000  \n",
       "1500           0.009162           0.118452  \n",
       "1501           0.027624           0.224458  \n",
       "1502           0.000736           0.023515  \n",
       "1503           0.000000           0.000000  \n",
       "1504           0.010250           0.107697  \n",
       "1505           0.000000           0.000000  \n",
       "1506           0.000000           0.000000  \n",
       "1507           0.003383           0.065527  \n",
       "1508           0.015849           0.136885  \n",
       "\n",
       "[1471 rows x 179 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1250, 75, 75, 3), (221, 75, 75, 3), (1250, 179), (221, 179), (1250,), (221,))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a train and validation split, 75% of data used in training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, X_features_train, X_features_valid, Y_train, Y_valid = train_test_split(Xtrain,\n",
    "                                    Xfeatures, Ytrain, random_state=666, train_size=0.85, test_size=0.15)\n",
    "\n",
    "X_train.shape, X_valid.shape, X_features_train.shape, X_features_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3750, 75, 75, 3), (3750, 179), (3750,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = get_more_images(X_train)\n",
    "X_features_train = np.concatenate((X_features_train,X_features_train,X_features_train))\n",
    "Y_train = np.concatenate((Y_train,Y_train,Y_train))\n",
    "\n",
    "X_train.shape, X_features_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "        \n",
    "tbCallBack = TensorBoard(log_dir='/home/ubuntu/data/tensorboardlogs/', histogram_freq=0, write_graph=True, write_images=True)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, epsilon=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "input_shape = (75, 75, 3)\n",
    "num_classes = 2\n",
    "\n",
    "classifier_input = Input(shape=input_shape)\n",
    "features_input = Input(shape=(num_features,))\n",
    "\n",
    "# CNN 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(classifier_input)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = MaxPooling2D()(x)   # REMOVED MAX POOLING FOR VISUALISATION\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "\n",
    "\n",
    "# CNN 4\n",
    "x = Conv2D(64,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "m = Concatenate()([features_input, x])\n",
    "m = Dense(512, activation='relu')(m)\n",
    "x = Dropout(p)(x)\n",
    "m = Dense(512, activation='relu')(m)\n",
    "x = Dropout(p)(x)\n",
    "m = Dense(512, activation='relu')(m)\n",
    "m = Dense(256, activation='sigmoid')(m)\n",
    "out = Dense(2, activation='sigmoid')(m)\n",
    "# out = Activation('softmax')(m)\n",
    "\n",
    "# optimizer = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "optimizer = Adam(lr=0.0001, decay=0.0)\n",
    "model = Model(inputs=[classifier_input, features_input], outputs=out)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# model.fit([X_train, X_features_train], Y_train,\n",
    "#                                           batch_size=128, \n",
    "#                                           epochs=50,\n",
    "#                                           validation_data=([X_valid, X_features_valid], Y_valid),\n",
    "#                                           shuffle=True, \n",
    "#                                           callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 221 samples\n",
      "Epoch 1/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.3856 - acc: 0.8216 - val_loss: 0.3150 - val_acc: 0.8846\n",
      "Epoch 2/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.3449 - acc: 0.8396 - val_loss: 0.2882 - val_acc: 0.8824\n",
      "Epoch 3/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.3298 - acc: 0.8504 - val_loss: 0.2606 - val_acc: 0.9050\n",
      "Epoch 4/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.3105 - acc: 0.8621 - val_loss: 0.2626 - val_acc: 0.8801\n",
      "Epoch 5/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2891 - acc: 0.8717 - val_loss: 0.2456 - val_acc: 0.8914\n",
      "Epoch 6/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2769 - acc: 0.8761 - val_loss: 0.2249 - val_acc: 0.8959\n",
      "Epoch 7/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2724 - acc: 0.8800 - val_loss: 0.2172 - val_acc: 0.9027\n",
      "Epoch 8/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2571 - acc: 0.8881 - val_loss: 0.2004 - val_acc: 0.8982\n",
      "Epoch 9/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2569 - acc: 0.8896 - val_loss: 0.2447 - val_acc: 0.8756\n",
      "Epoch 10/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2458 - acc: 0.8975 - val_loss: 0.2487 - val_acc: 0.8756\n",
      "Epoch 11/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2434 - acc: 0.8993 - val_loss: 0.2943 - val_acc: 0.8665\n",
      "Epoch 12/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2367 - acc: 0.9067 - val_loss: 0.2076 - val_acc: 0.9072\n",
      "Epoch 13/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2347 - acc: 0.9027 - val_loss: 0.2632 - val_acc: 0.8733\n",
      "Epoch 14/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2340 - acc: 0.9065 - val_loss: 0.2788 - val_acc: 0.8733\n",
      "Epoch 15/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2285 - acc: 0.9065 - val_loss: 0.2695 - val_acc: 0.8756\n",
      "Epoch 16/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2251 - acc: 0.9069 - val_loss: 0.2704 - val_acc: 0.8801\n",
      "Epoch 17/50\n",
      "3584/3750 [===========================>..] - ETA: 0s - loss: 0.2286 - acc: 0.9051\n",
      "Epoch 00017: reducing learning rate to 9.999999747378752e-06.\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2271 - acc: 0.9053 - val_loss: 0.2500 - val_acc: 0.8801\n",
      "Epoch 18/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2187 - acc: 0.9079 - val_loss: 0.1920 - val_acc: 0.9140\n",
      "Epoch 19/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2146 - acc: 0.9135 - val_loss: 0.2379 - val_acc: 0.8801\n",
      "Epoch 20/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2143 - acc: 0.9108 - val_loss: 0.1973 - val_acc: 0.9095\n",
      "Epoch 21/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2122 - acc: 0.9147 - val_loss: 0.2068 - val_acc: 0.9118\n",
      "Epoch 22/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2086 - acc: 0.9140 - val_loss: 0.2079 - val_acc: 0.9118\n",
      "Epoch 23/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2103 - acc: 0.9132 - val_loss: 0.2051 - val_acc: 0.9118\n",
      "Epoch 24/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2158 - acc: 0.9065 - val_loss: 0.2243 - val_acc: 0.9050\n",
      "Epoch 25/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2066 - acc: 0.9207 - val_loss: 0.1951 - val_acc: 0.9095\n",
      "Epoch 26/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2140 - acc: 0.9100 - val_loss: 0.2160 - val_acc: 0.9095\n",
      "Epoch 27/50\n",
      "3584/3750 [===========================>..] - ETA: 0s - loss: 0.2132 - acc: 0.9122\n",
      "Epoch 00027: reducing learning rate to 9.999999747378752e-07.\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2102 - acc: 0.9140 - val_loss: 0.2106 - val_acc: 0.9140\n",
      "Epoch 28/50\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2075 - acc: 0.9179 - val_loss: 0.2085 - val_acc: 0.9118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8385021470>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train, X_features_train], to_categorical(Y_train),\n",
    "                                          batch_size=256, \n",
    "                                          epochs=50,\n",
    "                                          validation_data=([X_valid, X_features_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print(K.get_value(model.optimizer.lr))\n",
    "K.set_value(model.optimizer.lr, .0001)\n",
    "print(K.get_value(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "print(K.get_value(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 221 samples\n",
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.2093 - acc: 0.9157 - val_loss: 0.2102 - val_acc: 0.9050\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.1867 - acc: 0.9275 - val_loss: 0.2037 - val_acc: 0.9118\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.1840 - acc: 0.9296 - val_loss: 0.2060 - val_acc: 0.9095\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.1793 - acc: 0.9321 - val_loss: 0.2083 - val_acc: 0.9095\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.1822 - acc: 0.9309 - val_loss: 0.2065 - val_acc: 0.9140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f85a9f27c18>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train, X_features_train], to_categorical(Y_train),\n",
    "                                          batch_size=256, \n",
    "                                          epochs=5,\n",
    "                                          validation_data=([X_valid, X_features_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/home/ubuntu/data/iceberg/results/weights/model_with_haralicks_094_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 75, 75, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 75, 75, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 37, 37, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 37, 37, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 18, 18, 128)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 18, 18, 128)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 18, 18, 128)  147584      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 18, 18, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 9, 9, 128)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 9, 9, 128)    0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 9, 9, 256)    295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 9, 9, 64)     147520      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 179)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 5184)         0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5363)         0           input_2[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000)         5364000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          512512      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          262656      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          131328      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            257         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,937,185\n",
      "Trainable params: 6,936,929\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizertimizerodel.fit([X_train, X_features_train], Y_train,\n",
    "                                          batch_size=128, \n",
    "                                          epochs=3,\n",
    "                                          validation_data=([X_valid, X_features_valid], Y_valid),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_dir = '/home/ubuntu/data/iceberg/results/final_preds/mix'\n",
    "pred_test = pd.read_csv(preds_dir + '/cnn_train_0.96_best.csv').as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/ubuntu/data/iceberg/results/weights/norm_image_inc_angle-Copy1_94_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1471/1471 [==============================] - 1s 629us/step\n",
      "Train score: 0.168463380644\n",
      "Train accuracy: 0.938137320942\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights(filepath = '.mdl_wts.hdf5')\n",
    "score = model.evaluate([Xtrain,Xfeatures], to_categorical(Ytrain), verbose=1)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "# Additional image features from David\n",
    "test_features = pd.read_csv(\"/home/ubuntu/data/iceberg/features/test.csv\", index_col=0)\n",
    "test_features.drop(labels=[\"inc_angle\"], inplace=True, axis=1)\n",
    "# #test_features = pd.read_csv(\"iceberg_features/test.csv\", index_col=0)\n",
    "\n",
    "df_test = pd.read_json(os.path.join(data_dir, 'test.json'))\n",
    "\n",
    "df_join_test = pd.merge(df_test, test_features, on='id', how=\"outer\")\n",
    "\n",
    "df_join_test.inc_angle = df_test.inc_angle.replace('na',0)\n",
    "Xtest = (get_scaled_imgs(df_join_test))\n",
    "Xtest_inc = df_join_test.inc_angle\n",
    "Xtest_features = df_join_test.drop(labels=[\"id\", \"band_1\", \"band_2\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_test = model.predict([Xtest,Xtest_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8424, 75, 75, 3), (8424,), (8424, 179), (8424, 2))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape, Xtest_inc.shape, Xtest_features.shape, pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "Xtest_features.iloc[:,1:179] = min_max_scaler.fit_transform(Xtest_features.iloc[:,1:179])\n",
    "Xtest_features = pd.DataFrame(Xtest_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtest_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8172, 75, 75, 3),\n",
       " (83, 75, 75, 3),\n",
       " (8172, 1, 179),\n",
       " (83, 1, 179),\n",
       " (8172, 2),\n",
       " (83, 2))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a train and validation split, 75% of data used in training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtest, X_valid2, Xtest_features, X_features_valid2, pred_test, Y_valid2 = train_test_split(Xtest,\n",
    "                                    Xtest_features, pred_test, random_state=666, train_size=0.99, test_size=0.01)\n",
    "\n",
    "Xtest.shape, X_valid2.shape, Xtest_features.shape, X_features_valid2.shape, pred_test.shape, Y_valid2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pseudo labelling using the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the correct way round with [:,1]\n",
    "idx_pred_1 = (np.where(pred_test[:,1]>0.9))\n",
    "idx_pred_0 = (np.where(pred_test[:,1]<0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8172, 1, 179), (1250, 1, 179))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_features.shape, X_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xtest_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtest_features= np.expand_dims(Xtest_features, axis=1)\n",
    "X_features_train = np.expand_dims(X_features_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.squeeze(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.expand_dims(Y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1250, 75, 75, 3), (1250,), (1250, 1, 179))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6389, 75, 75, 3), (6389,), (6389, 1, 179))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_pl = np.concatenate((X_train, Xtest[idx_pred_1[0],...], Xtest[idx_pred_0[0],...]))\n",
    "Ytrain_pl = np.concatenate((Y_train, np.ones(idx_pred_1[0].shape[0]), np.zeros(idx_pred_0[0].shape[0])))\n",
    "Xfeatures_pl = np.concatenate((X_features_train, Xtest_features[idx_pred_1[0],...], Xtest_features[idx_pred_0[0],...]))\n",
    "\n",
    "Xtrain_pl.shape, Ytrain_pl.shape, Xfeatures_pl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.36383000e+01,   5.00588553e-01,   4.67765101e-01, ...,\n",
       "          2.63998184e-01,   7.10061929e-02,   3.24395960e-01],\n",
       "       [  3.95560000e+01,   5.06960400e-01,   4.83753482e-01, ...,\n",
       "          2.47054153e-01,   3.24999004e-02,   1.93697333e-01],\n",
       "       [  4.11227000e+01,   5.04139506e-01,   5.08525256e-01, ...,\n",
       "          2.05253887e-01,   3.41996029e-02,   2.08852112e-01],\n",
       "       ..., \n",
       "       [  4.32964000e+01,   5.20551724e-01,   5.53583540e-01, ...,\n",
       "          1.83492614e-01,   1.47718640e-02,   1.39910668e-01],\n",
       "       [  4.28744222e+01,   2.59100829e-01,   4.44639244e-01, ...,\n",
       "          1.46653698e-01,   1.07354478e-01,   3.71863288e-01],\n",
       "       [  3.74889000e+01,   5.46588408e-01,   5.33905141e-01, ...,\n",
       "          1.71438966e-01,   6.78073835e-03,   8.38025895e-02]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xfeatures_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xfeatures_pl = np.squeeze(Xfeatures_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "input_shape = (75, 75, 3)\n",
    "num_classes = 2\n",
    "\n",
    "classifier_input = Input(shape=input_shape)\n",
    "features_input = Input(shape=(num_features,))\n",
    "\n",
    "# CNN 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(classifier_input)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = MaxPooling2D()(x)   # REMOVED MAX POOLING FOR VISUALISATION\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "\n",
    "\n",
    "# CNN 4\n",
    "x = Conv2D(64,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "m = Concatenate()([features_input, x])\n",
    "m = Dense(512, activation='relu')(m)\n",
    "x = Dropout(p)(x)\n",
    "m = Dense(512, activation='relu')(m)\n",
    "x = Dropout(p)(x)\n",
    "m = Dense(512, activation='relu')(m)\n",
    "m = Dense(256, activation='sigmoid')(m)\n",
    "out = Dense(2, activation='sigmoid')(m)\n",
    "# out = Activation('softmax')(m)\n",
    "\n",
    "# optimizer = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "optimizer = Adam(lr=0.0001, decay=0.0)\n",
    "model_p = Model(inputs=[classifier_input, features_input], outputs=out)\n",
    "model_p.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6389 samples, validate on 221 samples\n",
      "Epoch 1/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.4222 - acc: 0.8062 - val_loss: 0.4386 - val_acc: 0.8054\n",
      "Epoch 2/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.3940 - acc: 0.8255 - val_loss: 0.4124 - val_acc: 0.8213\n",
      "Epoch 3/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.3584 - acc: 0.8425 - val_loss: 0.3678 - val_acc: 0.8462\n",
      "Epoch 4/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.3216 - acc: 0.8627 - val_loss: 0.3142 - val_acc: 0.8620\n",
      "Epoch 5/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.2812 - acc: 0.8832 - val_loss: 0.2830 - val_acc: 0.8643\n",
      "Epoch 6/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.2606 - acc: 0.8924 - val_loss: 0.2612 - val_acc: 0.8801\n",
      "Epoch 7/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.2416 - acc: 0.9021 - val_loss: 0.2513 - val_acc: 0.8801\n",
      "Epoch 8/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.2198 - acc: 0.9111 - val_loss: 0.2496 - val_acc: 0.8914\n",
      "Epoch 9/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.2101 - acc: 0.9150 - val_loss: 0.2226 - val_acc: 0.9027\n",
      "Epoch 10/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1933 - acc: 0.9216 - val_loss: 0.2145 - val_acc: 0.9005\n",
      "Epoch 11/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1819 - acc: 0.9304 - val_loss: 0.2217 - val_acc: 0.8982\n",
      "Epoch 12/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1731 - acc: 0.9311 - val_loss: 0.2085 - val_acc: 0.9050\n",
      "Epoch 13/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1733 - acc: 0.9295 - val_loss: 0.2295 - val_acc: 0.8959\n",
      "Epoch 14/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1661 - acc: 0.9333 - val_loss: 0.2018 - val_acc: 0.9140\n",
      "Epoch 15/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1548 - acc: 0.9411 - val_loss: 0.2116 - val_acc: 0.9118\n",
      "Epoch 16/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1560 - acc: 0.9411 - val_loss: 0.2131 - val_acc: 0.9186\n",
      "Epoch 17/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1495 - acc: 0.9454 - val_loss: 0.1877 - val_acc: 0.9231\n",
      "Epoch 18/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1439 - acc: 0.9435 - val_loss: 0.1999 - val_acc: 0.9186\n",
      "Epoch 19/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1382 - acc: 0.9474 - val_loss: 0.2322 - val_acc: 0.9050\n",
      "Epoch 20/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1348 - acc: 0.9490 - val_loss: 0.2247 - val_acc: 0.8959\n",
      "Epoch 21/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1356 - acc: 0.9478 - val_loss: 0.2660 - val_acc: 0.9050\n",
      "Epoch 22/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1313 - acc: 0.9476 - val_loss: 0.1946 - val_acc: 0.9321\n",
      "Epoch 23/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1197 - acc: 0.9535 - val_loss: 0.2099 - val_acc: 0.9140\n",
      "Epoch 24/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1173 - acc: 0.9573 - val_loss: 0.1876 - val_acc: 0.9276\n",
      "Epoch 25/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1154 - acc: 0.9575 - val_loss: 0.2108 - val_acc: 0.9140\n",
      "Epoch 26/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1225 - acc: 0.9527 - val_loss: 0.2267 - val_acc: 0.9095\n",
      "Epoch 27/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1254 - acc: 0.9532 - val_loss: 0.1902 - val_acc: 0.9321\n",
      "Epoch 28/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1085 - acc: 0.9609 - val_loss: 0.2128 - val_acc: 0.9118\n",
      "Epoch 29/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1089 - acc: 0.9601 - val_loss: 0.1845 - val_acc: 0.9276\n",
      "Epoch 30/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1112 - acc: 0.9559 - val_loss: 0.1728 - val_acc: 0.9321\n",
      "Epoch 31/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1142 - acc: 0.9584 - val_loss: 0.1729 - val_acc: 0.9457\n",
      "Epoch 32/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1128 - acc: 0.9580 - val_loss: 0.1893 - val_acc: 0.9321\n",
      "Epoch 33/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0985 - acc: 0.9620 - val_loss: 0.1837 - val_acc: 0.9389\n",
      "Epoch 34/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.1002 - acc: 0.9618 - val_loss: 0.1839 - val_acc: 0.9367\n",
      "Epoch 35/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0919 - acc: 0.9672 - val_loss: 0.1837 - val_acc: 0.9412\n",
      "Epoch 36/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0961 - acc: 0.9635 - val_loss: 0.1635 - val_acc: 0.9367\n",
      "Epoch 37/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0897 - acc: 0.9677 - val_loss: 0.1708 - val_acc: 0.9412\n",
      "Epoch 38/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0862 - acc: 0.9689 - val_loss: 0.1955 - val_acc: 0.9321\n",
      "Epoch 39/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0853 - acc: 0.9691 - val_loss: 0.1781 - val_acc: 0.9412\n",
      "Epoch 40/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0868 - acc: 0.9661 - val_loss: 0.1724 - val_acc: 0.9457\n",
      "Epoch 41/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0914 - acc: 0.9638 - val_loss: 0.2253 - val_acc: 0.9276\n",
      "Epoch 42/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0930 - acc: 0.9662 - val_loss: 0.2305 - val_acc: 0.9231\n",
      "Epoch 43/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0838 - acc: 0.9694 - val_loss: 0.1823 - val_acc: 0.9321\n",
      "Epoch 44/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0839 - acc: 0.9688 - val_loss: 0.2057 - val_acc: 0.9321\n",
      "Epoch 45/50\n",
      "6144/6389 [===========================>..] - ETA: 0s - loss: 0.0790 - acc: 0.9696\n",
      "Epoch 00045: reducing learning rate to 9.999999747378752e-06.\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0773 - acc: 0.9703 - val_loss: 0.2114 - val_acc: 0.9367\n",
      "Epoch 46/50\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0770 - acc: 0.9710 - val_loss: 0.1962 - val_acc: 0.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84f65d10b8>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_p.fit([Xtrain_pl, Xfeatures_pl], to_categorical(Ytrain_pl),\n",
    "                                          batch_size=256, \n",
    "                                          epochs=50,\n",
    "                                          validation_data=([X_valid, X_features_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print(K.get_value(model_p.optimizer.lr))\n",
    "K.set_value(model_p.optimizer.lr, .0001)\n",
    "print(K.get_value(model_p.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6389 samples, validate on 221 samples\n",
      "Epoch 1/2\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0365 - acc: 0.9890 - val_loss: 0.1485 - val_acc: 0.9502\n",
      "Epoch 2/2\n",
      "6389/6389 [==============================] - 10s 2ms/step - loss: 0.0363 - acc: 0.9887 - val_loss: 0.1491 - val_acc: 0.9457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84966df860>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_p.fit([Xtrain_pl, Xfeatures_pl], to_categorical(Ytrain_pl),\n",
    "                                          batch_size=256, \n",
    "                                          epochs=2,\n",
    "                                          validation_data=([X_valid, X_features_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/ubuntu/data/iceberg/results/weights/haralicks_model_acc95.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1471/1471 [==============================] - 1s 670us/step\n",
      "Train score: 0.11513602976\n",
      "Train accuracy: 0.961930658808\n"
     ]
    }
   ],
   "source": [
    "score = model_p.evaluate([Xtrain,Xfeatures], to_categorical(Ytrain), verbose=1)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "# Additional image features from David\n",
    "test_features = pd.read_csv(\"/home/ubuntu/data/iceberg/features/test.csv\", index_col=0)\n",
    "test_features.drop(labels=[\"inc_angle\"], inplace=True, axis=1)\n",
    "# #test_features = pd.read_csv(\"iceberg_features/test.csv\", index_col=0)\n",
    "\n",
    "df_test = pd.read_json(os.path.join(data_dir, 'test.json'))\n",
    "\n",
    "df_join_test = pd.merge(df_test, test_features, on='id', how=\"outer\")\n",
    "\n",
    "df_join_test.inc_angle = df_test.inc_angle.replace('na',0)\n",
    "Xtest = (get_scaled_imgs(df_join_test))\n",
    "Xtest_inc = df_join_test.inc_angle\n",
    "Xtest_features = df_join_test.drop(labels=[\"id\", \"band_1\", \"band_2\"], axis=1)\n",
    "\n",
    "pred_test = model_p.predict([Xtest,Xtest_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8424, 75, 75, 3), (8424, 2))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape, pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit_nums = np.clip(pred_test[:,1], 0.1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  is_iceberg\n",
      "0  5941774d         0.1\n",
      "1  4023181e         0.9\n",
      "2  b20200e4         0.1\n",
      "3  e7f018bb         0.1\n",
      "4  4371c8c3         0.1\n",
      "5  a8d9b1fd         0.9\n",
      "6  29e7727e         0.1\n",
      "7  92a51ffb         0.1\n",
      "8  c769ac97         0.1\n",
      "9  aee0547d         0.1\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': submit_nums.reshape((pred_test[:,1].shape[0]))})\n",
    "print(submission.head(10))\n",
    "\n",
    "submission.to_csv('cnn_with_haralicks_clipped' + str(np.around(score[1], decimals=2))  + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/git/learningWithKaggle/ice'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "input_shape = (75, 75, 3)\n",
    "num_classes = 2\n",
    "\n",
    "classifier_input = Input(shape=input_shape)\n",
    "inc_angle_input = Input(shape=(1,))\n",
    "\n",
    "# CNN 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(classifier_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)   # REMOVED MAX POOLING FOR VISUALISATION\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "\n",
    "# CNN 4\n",
    "x = Conv2D(64,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "m = Concatenate()([inc_angle_input, x])\n",
    "m = Dense(512, activation='relu')(m)\n",
    "m = Dense(256, activation='relu')(m)\n",
    "out = Dense(2, activation='sigmoid')(m)\n",
    "# out = Activation('softmax')(m)\n",
    "\n",
    "# optimizer = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "optimizer = Adam(lr=0.0015, decay=0.0)\n",
    "model2 = Model(inputs=[classifier_input, inc_angle_input], outputs=out)\n",
    "model2.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9763 samples, validate on 148 samples\n",
      "Epoch 1/50\n",
      "9763/9763 [==============================] - 16s 2ms/step - loss: 0.6166 - acc: 0.6713 - val_loss: 0.6229 - val_acc: 0.7128\n",
      "Epoch 2/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.3354 - acc: 0.8669 - val_loss: 0.5192 - val_acc: 0.7770\n",
      "Epoch 3/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.2501 - acc: 0.9042 - val_loss: 0.4833 - val_acc: 0.7939 loss: 0.2537 - acc\n",
      "Epoch 4/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.2342 - acc: 0.9029 - val_loss: 0.2614 - val_acc: 0.8716\n",
      "Epoch 5/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.1608 - acc: 0.9301 - val_loss: 0.2916 - val_acc: 0.8649\n",
      "Epoch 6/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.1422 - acc: 0.9369 - val_loss: 0.2314 - val_acc: 0.8986\n",
      "Epoch 7/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.1399 - acc: 0.9404 - val_loss: 0.2208 - val_acc: 0.9054\n",
      "Epoch 8/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.1325 - acc: 0.9407 - val_loss: 0.1957 - val_acc: 0.9189\n",
      "Epoch 9/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.1289 - acc: 0.9458 - val_loss: 0.1671 - val_acc: 0.9324 - loss: 0.1287 - acc: 0.946\n",
      "Epoch 10/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.1017 - acc: 0.9579 - val_loss: 0.1600 - val_acc: 0.9392\n",
      "Epoch 11/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.1026 - acc: 0.9592 - val_loss: 0.2405 - val_acc: 0.8919\n",
      "Epoch 12/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0954 - acc: 0.9640 - val_loss: 0.1910 - val_acc: 0.9426\n",
      "Epoch 13/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0901 - acc: 0.9655 - val_loss: 0.2145 - val_acc: 0.8986\n",
      "Epoch 14/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0888 - acc: 0.9667 - val_loss: 0.1794 - val_acc: 0.9088\n",
      "Epoch 15/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0872 - acc: 0.9643 - val_loss: 0.1703 - val_acc: 0.9189\n",
      "Epoch 16/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0786 - acc: 0.9681 - val_loss: 0.1504 - val_acc: 0.9392\n",
      "Epoch 17/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0843 - acc: 0.9686 - val_loss: 0.1670 - val_acc: 0.9257\n",
      "Epoch 18/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0755 - acc: 0.9703 - val_loss: 0.1800 - val_acc: 0.9392\n",
      "Epoch 19/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0765 - acc: 0.9700 - val_loss: 0.1476 - val_acc: 0.9493\n",
      "Epoch 20/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0699 - acc: 0.9719 - val_loss: 0.2285 - val_acc: 0.9088: 8s - loss: 0.0692 - acc: 0. \n",
      "Epoch 21/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0869 - acc: 0.9654 - val_loss: 0.1955 - val_acc: 0.9088\n",
      "Epoch 22/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0681 - acc: 0.9743 - val_loss: 0.2343 - val_acc: 0.8581\n",
      "Epoch 23/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0739 - acc: 0.9713 - val_loss: 0.1877 - val_acc: 0.8986\n",
      "Epoch 24/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0705 - acc: 0.9728 - val_loss: 0.1635 - val_acc: 0.9459\n",
      "Epoch 25/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0596 - acc: 0.9769 - val_loss: 0.1316 - val_acc: 0.9324\n",
      "Epoch 26/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0594 - acc: 0.9773 - val_loss: 0.1956 - val_acc: 0.8986\n",
      "Epoch 27/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0675 - acc: 0.9725 - val_loss: 0.1985 - val_acc: 0.9561\n",
      "Epoch 28/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0559 - acc: 0.9783 - val_loss: 0.2392 - val_acc: 0.9324\n",
      "Epoch 29/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0567 - acc: 0.9779 - val_loss: 0.1464 - val_acc: 0.9291\n",
      "Epoch 30/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0507 - acc: 0.9802 - val_loss: 0.1435 - val_acc: 0.9527\n",
      "Epoch 31/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0486 - acc: 0.9820 - val_loss: 0.2042 - val_acc: 0.9155\n",
      "Epoch 32/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0590 - acc: 0.9784 - val_loss: 0.1604 - val_acc: 0.9324\n",
      "Epoch 33/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0431 - acc: 0.9840 - val_loss: 0.1138 - val_acc: 0.9561\n",
      "Epoch 34/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0491 - acc: 0.9820 - val_loss: 0.1949 - val_acc: 0.9392\n",
      "Epoch 35/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0347 - acc: 0.9855 - val_loss: 0.1416 - val_acc: 0.9730\n",
      "Epoch 36/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0342 - acc: 0.9870 - val_loss: 0.2446 - val_acc: 0.8919\n",
      "Epoch 37/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0346 - acc: 0.9875 - val_loss: 0.1593 - val_acc: 0.9324\n",
      "Epoch 38/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0394 - acc: 0.9853 - val_loss: 0.1631 - val_acc: 0.9459\n",
      "Epoch 39/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0364 - acc: 0.9862 - val_loss: 0.1719 - val_acc: 0.9257\n",
      "Epoch 40/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0423 - acc: 0.9848 - val_loss: 0.1674 - val_acc: 0.9257\n",
      "Epoch 41/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0429 - acc: 0.9852 - val_loss: 0.1439 - val_acc: 0.9324\n",
      "Epoch 42/50\n",
      "9728/9763 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9903\n",
      "Epoch 00042: reducing learning rate to 0.00015000000130385163.\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0271 - acc: 0.9904 - val_loss: 0.2196 - val_acc: 0.9459\n",
      "Epoch 43/50\n",
      "9763/9763 [==============================] - 15s 2ms/step - loss: 0.0238 - acc: 0.9921 - val_loss: 0.1928 - val_acc: 0.9324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd83abb9780>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([Xtrain_pl, Xinc_pl], to_categorical(Ytrain_pl),\n",
    "                                          batch_size=128, \n",
    "                                          epochs=50,\n",
    "                                          validation_data=([X_valid, X_inc_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as well as adding the pseudo labelling, augment the pseudo labelled data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29289, 75, 75, 3), (29289,), (29289,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this way will reaugment the orginal images, perhaps doesn't matter? \n",
    "\n",
    "Xtrain_pl = get_more_images(Xtrain_pl)\n",
    "Xinc_pl = np.concatenate((Xinc_pl,Xinc_pl,Xinc_pl))\n",
    "Ytrain_pl = np.concatenate((Ytrain_pl,Ytrain_pl,Ytrain_pl))\n",
    "\n",
    "Xtrain_pl.shape, Ytrain_pl.shape, Xinc_pl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29289 samples, validate on 148 samples\n",
      "Epoch 1/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0408 - acc: 0.9864 - val_loss: 0.1343 - val_acc: 0.9561\n",
      "Epoch 2/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0367 - acc: 0.9876 - val_loss: 0.1561 - val_acc: 0.9459\n",
      "Epoch 3/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0316 - acc: 0.9882 - val_loss: 0.1464 - val_acc: 0.9662\n",
      "Epoch 4/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0343 - acc: 0.9881 - val_loss: 0.1358 - val_acc: 0.9595 acc: 0.98 - ETA: 5s - loss: 0.0344 - acc: 0.988 - ETA: 4s - loss: 0.0344 - acc: 0.987 - ETA: 4s - lo\n",
      "Epoch 5/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0291 - acc: 0.9892 - val_loss: 0.1477 - val_acc: 0.9595\n",
      "Epoch 6/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0299 - acc: 0.9893 - val_loss: 0.1663 - val_acc: 0.9527\n",
      "Epoch 7/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0278 - acc: 0.9900 - val_loss: 0.1594 - val_acc: 0.9595: 6s - loss: 0.0276 - acc: 0.99 - ETA: 5s - lo - ETA: 1s - loss: 0.0277 - acc: \n",
      "Epoch 8/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0257 - acc: 0.9907 - val_loss: 0.1730 - val_acc: 0.9459\n",
      "Epoch 9/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0242 - acc: 0.9910 - val_loss: 0.1678 - val_acc: 0.9662\n",
      "Epoch 10/50\n",
      "29184/29289 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9916\n",
      "Epoch 00010: reducing learning rate to 1.500000071246177e-05.\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0232 - acc: 0.9917 - val_loss: 0.1704 - val_acc: 0.9595\n",
      "Epoch 11/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0210 - acc: 0.9923 - val_loss: 0.1718 - val_acc: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd83aa742b0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([Xtrain_pl, Xinc_pl], to_categorical(Ytrain_pl),\n",
    "                                          batch_size=128, \n",
    "                                          epochs=50,\n",
    "                                          validation_data=([X_valid, X_inc_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 435 and 257 for 'Assign_10' (op: 'Assign') with input shapes: [435,512], [257,512].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 435 and 257 for 'Assign_10' (op: 'Assign') with input shapes: [435,512], [257,512].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dd2a591869e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/ubuntu/data/iceberg/results/weights/norm_image_inc_angle-Copy1_model2_95_acc.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2620\u001b[0m             \u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   3140\u001b[0m                              ' elements.')\n\u001b[1;32m   3141\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2241\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2242\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking)\u001b[0m\n\u001b[1;32m    520\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     45\u001b[0m   result = _op_def_lib.apply_op(\"Assign\", ref=ref, value=value,\n\u001b[1;32m     46\u001b[0m                                 \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                                 use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 435 and 257 for 'Assign_10' (op: 'Assign') with input shapes: [435,512], [257,512]."
     ]
    }
   ],
   "source": [
    "model.load_weights('/home/ubuntu/data/iceberg/results/weights/norm_image_inc_angle-Copy1_model2_95_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xtest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-945ec111c9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest_inc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Xtest' is not defined"
     ]
    }
   ],
   "source": [
    "Xtest.shape, Xtest_inc.shape, pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25272, 75, 75, 3), (25272,), (25272, 2))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest = get_more_images(Xtest)\n",
    "Xtest_inc = np.concatenate((Xtest_inc,Xtest_inc,Xtest_inc))\n",
    "pred_test = np.concatenate((pred_test, pred_test, pred_test))\n",
    "\n",
    "Xtest.shape, Xtest_inc.shape, pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the correct way round with [:,1]\n",
    "idx_pred_1 = (np.where(pred_test[:,1]>0.95))\n",
    "idx_pred_0 = (np.where(pred_test[:,1]<0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xtest_inc = np.expand_dims(Xtest_inc, axis=1)\n",
    "X_inc_train = np.expand_dims(X_inc_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3969, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_inc_train = np.squeeze(X_inc_train)\n",
    "X_inc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21351, 75, 75, 3), (21351,), (21351, 1))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_pl = np.concatenate((X_train, Xtest[idx_pred_1[0],...], Xtest[idx_pred_0[0],...]))\n",
    "Ytrain_pl = np.concatenate((Y_train, np.ones(idx_pred_1[0].shape[0]), np.zeros(idx_pred_0[0].shape[0])))\n",
    "Xinc_pl = np.concatenate((X_inc_train, Xtest_inc[idx_pred_1[0],...], Xtest_inc[idx_pred_0[0],...]))\n",
    "\n",
    "Xtrain_pl.shape, Ytrain_pl.shape, Xinc_pl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.4\n",
    "input_shape = (75, 75, 3)\n",
    "num_classes = 2\n",
    "\n",
    "classifier_input = Input(shape=input_shape)\n",
    "inc_angle_input = Input(shape=(1,))\n",
    "\n",
    "# CNN 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(classifier_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)   # REMOVED MAX POOLING FOR VISUALISATION\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "\n",
    "# CNN 4\n",
    "x = Conv2D(64,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = Dropout(p/2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "m = Concatenate()([inc_angle_input, x])\n",
    "m = Dense(512, activation='relu')(m)\n",
    "m = Dense(256, activation='relu')(m)\n",
    "out = Dense(2, activation='sigmoid')(m)\n",
    "# out = Activation('softmax')(m)\n",
    "\n",
    "# optimizer = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "optimizer = Adam(lr=0.0015, decay=0.0)\n",
    "model3 = Model(inputs=[classifier_input, inc_angle_input], outputs=out)\n",
    "model3.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xtrain_pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-71107d8f1468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model3.fit([Xtrain_pl, Xinc_pl], to_categorical(Ytrain_pl),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_inc_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Xtrain_pl' is not defined"
     ]
    }
   ],
   "source": [
    "model3.fit([Xtrain_pl, Xinc_pl], to_categorical(Ytrain_pl),\n",
    "                                          batch_size=256, \n",
    "                                          epochs=50,\n",
    "                                          validation_data=([X_valid, X_inc_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save_weights('/home/ubuntu/data/iceberg/results/weights/norm_image_inc_angle-Copy1_model2_95_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89999998"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit_nums = np.clip(pred_test[:,1], 0.1, 0.9)\n",
    "submit_nums[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  is_iceberg\n",
      "0  5941774d    0.001687\n",
      "1  4023181e    0.952506\n",
      "2  b20200e4    0.778264\n",
      "3  e7f018bb    0.997760\n",
      "4  4371c8c3    0.273599\n",
      "5  a8d9b1fd    0.829154\n",
      "6  29e7727e    0.055702\n",
      "7  92a51ffb    0.998996\n",
      "8  c769ac97    0.000014\n",
      "9  aee0547d    0.000027\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test[:,1].reshape((pred_test[:,1].shape[0]))})\n",
    "print(submission.head(10))\n",
    "\n",
    "submission.to_csv('cnn_train_' + str(np.around(score[1], decimals=2))  + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 4)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4413/4413 [==============================] - 3s 600us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13649047835023664, 0.95014729196998826]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([Xtrain,Xinc], to_categorical(Ytrain), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
