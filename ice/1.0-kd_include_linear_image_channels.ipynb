{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/home/ubuntu/data/iceberg'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Conv2D, Cropping2D\n",
    "from keras.layers import MaxPooling2D, ZeroPadding2D, BatchNormalization, Activation\n",
    "from keras.layers.merge import Add, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import cv2\n",
    "import keras\n",
    "import os\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras import __version__\n",
    "print(__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "\n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "#         imgs.append(np.dstack((band_1, band_2, band_3)))\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)\n",
    "\n",
    "def get_more_images(imgs):\n",
    "    \n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        a=imgs[i,:,:,0]\n",
    "        b=imgs[i,:,:,1]\n",
    "        c=imgs[i,:,:,2]\n",
    "        \n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        bv=cv2.flip(b,1)\n",
    "        bh=cv2.flip(b,0)\n",
    "        cv=cv2.flip(c,1)\n",
    "        ch=cv2.flip(c,0)\n",
    "        \n",
    "        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n",
    "        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h))\n",
    "    \n",
    "    return more_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_json(os.path.join(data_dir, 'train.json'))\n",
    "Xtrain = get_scaled_imgs(df_train)\n",
    "Ytrain = np.array(df_train['is_iceberg'])\n",
    "\n",
    "df_train.inc_angle = df_train.inc_angle.replace('na',0)\n",
    "idx_tr = np.where(df_train.inc_angle>0)\n",
    "\n",
    "Ytrain = Ytrain[idx_tr[0]]\n",
    "Xtrain = Xtrain[idx_tr[0],...]\n",
    "Xinc = df_train.inc_angle[idx_tr[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72976856454753891"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[1,:,:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1323, 75, 75, 3), (148, 75, 75, 3), (1323,), (148,), (1323,), (148,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a train and validation split, 75% of data used in training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, X_inc_train, X_inc_valid, Y_train, Y_valid = train_test_split(Xtrain,\n",
    "                                    Xinc, Ytrain, random_state=777, train_size=0.9, test_size=0.1)\n",
    "\n",
    "X_train.shape, X_valid.shape, X_inc_train.shape, X_inc_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3969, 75, 75, 3), (3969,), (3969,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = get_more_images(X_train)\n",
    "X_inc_train = np.concatenate((X_inc_train,X_inc_train,X_inc_train))\n",
    "Y_train = np.concatenate((Y_train,Y_train,Y_train))\n",
    "\n",
    "X_train.shape, X_inc_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "        \n",
    "tbCallBack = TensorBoard(log_dir='/home/ubuntu/data/tensorboardlogs/', histogram_freq=0, write_graph=True, write_images=True)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, epsilon=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "input_shape = (75, 75, 3)\n",
    "num_classes = 2\n",
    "\n",
    "classifier_input = Input(shape=input_shape)\n",
    "inc_angle_input = Input(shape=(1,))\n",
    "\n",
    "# CNN 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(classifier_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)   # REMOVED MAX POOLING FOR VISUALISATION\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "\n",
    "# CNN 4\n",
    "x = Conv2D(64,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "m = Concatenate()([inc_angle_input, x])\n",
    "m = Dense(512, activation='relu')(m)\n",
    "m = Dense(256, activation='relu')(m)\n",
    "out = Dense(2, activation='sigmoid')(m)\n",
    "# out = Activation('softmax')(m)\n",
    "\n",
    "# optimizer = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "optimizer = Adam(lr=0.0015, decay=0.0)\n",
    "model = Model(inputs=[classifier_input, inc_angle_input], outputs=out)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3969 samples, validate on 148 samples\n",
      "Epoch 1/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0892 - acc: 0.9680 - val_loss: 0.3063 - val_acc: 0.9189\n",
      "Epoch 2/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0868 - acc: 0.9660 - val_loss: 0.3030 - val_acc: 0.9257\n",
      "Epoch 3/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0813 - acc: 0.9746 - val_loss: 0.3066 - val_acc: 0.9324\n",
      "Epoch 4/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0794 - acc: 0.9729 - val_loss: 0.3125 - val_acc: 0.9257\n",
      "Epoch 5/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0812 - acc: 0.9694 - val_loss: 0.3245 - val_acc: 0.9392\n",
      "Epoch 6/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0708 - acc: 0.9730 - val_loss: 0.3197 - val_acc: 0.9426\n",
      "Epoch 7/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0776 - acc: 0.9728 - val_loss: 0.3066 - val_acc: 0.9392\n",
      "Epoch 8/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0695 - acc: 0.9769 - val_loss: 0.3123 - val_acc: 0.9392\n",
      "Epoch 9/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0689 - acc: 0.9754 - val_loss: 0.3228 - val_acc: 0.9392\n",
      "Epoch 10/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0739 - acc: 0.9742 - val_loss: 0.3281 - val_acc: 0.9459\n",
      "Epoch 11/50\n",
      "3968/3969 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9757- ETA: 0s - loss: 0.0671 - acc: 0.97\n",
      "Epoch 00011: reducing learning rate to 1.500000071246177e-05.\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0665 - acc: 0.9757 - val_loss: 0.3184 - val_acc: 0.9459\n",
      "Epoch 12/50\n",
      "3969/3969 [==============================] - 6s 2ms/step - loss: 0.0669 - acc: 0.9773 - val_loss: 0.3157 - val_acc: 0.9459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00a776d5c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit([X_train, X_inc_train], to_categorical(Y_train),\n",
    "                                          batch_size=128, \n",
    "                                          epochs=50,\n",
    "                                          validation_data=([X_valid, X_inc_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('/home/ubuntu/data/iceberg/results/weights/norm_image_inc_angle-data2_94_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1471/1471 [==============================] - 1s 647us/step\n",
      "Train score: 0.0566026721131\n",
      "Train accuracy: 0.987763426241\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights(filepath = '.mdl_wts.hdf5')\n",
    "score = model.evaluate([Xtrain,Xinc], to_categorical(Ytrain), verbose=1)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "df_test = pd.read_json(os.path.join(data_dir, 'test.json'))\n",
    "df_test.inc_angle = df_test.inc_angle.replace('na',0)\n",
    "Xtest = (get_scaled_imgs(df_test))\n",
    "Xtest_inc = df_test.inc_angle\n",
    "pred_test = model.predict([Xtest,Xtest_inc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id    is_iceberg\n",
      "0  5941774d  2.231949e-03\n",
      "1  4023181e  9.414673e-01\n",
      "2  b20200e4  9.350348e-01\n",
      "3  e7f018bb  9.999995e-01\n",
      "4  4371c8c3  1.864136e-01\n",
      "5  a8d9b1fd  9.991841e-01\n",
      "6  29e7727e  3.206558e-03\n",
      "7  92a51ffb  9.999993e-01\n",
      "8  c769ac97  3.583631e-12\n",
      "9  aee0547d  8.747134e-11\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test[:,1].reshape((pred_test[:,1].shape[0]))})\n",
    "print(submission.head(10))\n",
    "\n",
    "submission.to_csv('/home/ubuntu/data/iceberg/results/final_preds/cnn_model1_aug_data2' + str(np.around(score[1], decimals=2))  + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pseudo labelling using the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the correct way round with [:,1]\n",
    "idx_pred_1 = (np.where(pred_test[:,1]>0.95))\n",
    "idx_pred_0 = (np.where(pred_test[:,1]<0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8424,), (3969,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_inc.shape, X_inc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest_inc = np.expand_dims(Xtest_inc, axis=1)\n",
    "X_inc_train = np.expand_dims(X_inc_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10203, 75, 75, 3), (10203,), (10203, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_pl = np.concatenate((X_train, Xtest[idx_pred_1[0],...], Xtest[idx_pred_0[0],...]))\n",
    "Ytrain_pl = np.concatenate((Y_train, np.ones(idx_pred_1[0].shape[0]), np.zeros(idx_pred_0[0].shape[0])))\n",
    "Xinc_pl = np.concatenate((X_inc_train, Xtest_inc[idx_pred_1[0],...], Xtest_inc[idx_pred_0[0],...]))\n",
    "\n",
    "Xtrain_pl.shape, Ytrain_pl.shape, Xinc_pl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xinc_pl = np.squeeze(Xinc_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "input_shape = (75, 75, 3)\n",
    "num_classes = 2\n",
    "\n",
    "classifier_input = Input(shape=input_shape)\n",
    "inc_angle_input = Input(shape=(1,))\n",
    "\n",
    "# CNN 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(classifier_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)   # REMOVED MAX POOLING FOR VISUALISATION\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "\n",
    "# CNN 4\n",
    "x = Conv2D(64,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "m = Concatenate()([inc_angle_input, x])\n",
    "m = Dense(512, activation='relu')(m)\n",
    "m = Dense(256, activation='relu')(m)\n",
    "out = Dense(2, activation='sigmoid')(m)\n",
    "# out = Activation('softmax')(m)\n",
    "\n",
    "# optimizer = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "optimizer = Adam(lr=0.0015, decay=0.0)\n",
    "model2 = Model(inputs=[classifier_input, inc_angle_input], outputs=out)\n",
    "model2.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10203 samples, validate on 148 samples\n",
      "Epoch 1/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0550 - acc: 0.9799 - val_loss: 0.2761 - val_acc: 0.9291\n",
      "Epoch 2/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0534 - acc: 0.9802 - val_loss: 0.2824 - val_acc: 0.9223\n",
      "Epoch 3/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0514 - acc: 0.9810 - val_loss: 0.2703 - val_acc: 0.9223\n",
      "Epoch 4/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0529 - acc: 0.9815 - val_loss: 0.2788 - val_acc: 0.9223\n",
      "Epoch 5/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0505 - acc: 0.9805 - val_loss: 0.2680 - val_acc: 0.9324\n",
      "Epoch 6/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0471 - acc: 0.9833 - val_loss: 0.2869 - val_acc: 0.9223\n",
      "Epoch 7/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0456 - acc: 0.9830 - val_loss: 0.3149 - val_acc: 0.9257\n",
      "Epoch 8/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0463 - acc: 0.9821 - val_loss: 0.2920 - val_acc: 0.9189\n",
      "Epoch 9/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0471 - acc: 0.9825 - val_loss: 0.2952 - val_acc: 0.9189cc: 0. - ETA: 3s - loss: 0.0496 -  - ETA: 1s - loss: 0.0475 - acc:\n",
      "Epoch 10/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0436 - acc: 0.9841 - val_loss: 0.3117 - val_acc: 0.9189\n",
      "Epoch 11/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0464 - acc: 0.9824 - val_loss: 0.2991 - val_acc: 0.9257A: 0s - loss: 0.0469 - acc: 0.98\n",
      "Epoch 12/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0441 - acc: 0.9843 - val_loss: 0.3182 - val_acc: 0.9392\n",
      "Epoch 13/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0389 - acc: 0.9855 - val_loss: 0.2921 - val_acc: 0.9223\n",
      "Epoch 14/50\n",
      "10112/10203 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9846\n",
      "Epoch 00014: reducing learning rate to 1.500000071246177e-05.\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0423 - acc: 0.9845 - val_loss: 0.3009 - val_acc: 0.9189\n",
      "Epoch 15/50\n",
      "10203/10203 [==============================] - 16s 2ms/step - loss: 0.0372 - acc: 0.9857 - val_loss: 0.3029 - val_acc: 0.9189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00a776e8d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([Xtrain_pl, Xinc_pl], to_categorical(Ytrain_pl),\n",
    "                                          batch_size=128, \n",
    "                                          epochs=50,\n",
    "                                          validation_data=([X_valid, X_inc_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save_weights('/home/ubuntu/data/iceberg/results/weights/norm_image_inc_angle-data2_92_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as well as adding the pseudo labelling, augment the pseudo labelled data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29289, 75, 75, 3), (29289,), (29289,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this way will reaugment the orginal images, perhaps doesn't matter? \n",
    "\n",
    "Xtrain_pl = get_more_images(Xtrain_pl)\n",
    "Xinc_pl = np.concatenate((Xinc_pl,Xinc_pl,Xinc_pl))\n",
    "Ytrain_pl = np.concatenate((Ytrain_pl,Ytrain_pl,Ytrain_pl))\n",
    "\n",
    "Xtrain_pl.shape, Ytrain_pl.shape, Xinc_pl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29289 samples, validate on 148 samples\n",
      "Epoch 1/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0408 - acc: 0.9864 - val_loss: 0.1343 - val_acc: 0.9561\n",
      "Epoch 2/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0367 - acc: 0.9876 - val_loss: 0.1561 - val_acc: 0.9459\n",
      "Epoch 3/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0316 - acc: 0.9882 - val_loss: 0.1464 - val_acc: 0.9662\n",
      "Epoch 4/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0343 - acc: 0.9881 - val_loss: 0.1358 - val_acc: 0.9595 acc: 0.98 - ETA: 5s - loss: 0.0344 - acc: 0.988 - ETA: 4s - loss: 0.0344 - acc: 0.987 - ETA: 4s - lo\n",
      "Epoch 5/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0291 - acc: 0.9892 - val_loss: 0.1477 - val_acc: 0.9595\n",
      "Epoch 6/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0299 - acc: 0.9893 - val_loss: 0.1663 - val_acc: 0.9527\n",
      "Epoch 7/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0278 - acc: 0.9900 - val_loss: 0.1594 - val_acc: 0.9595: 6s - loss: 0.0276 - acc: 0.99 - ETA: 5s - lo - ETA: 1s - loss: 0.0277 - acc: \n",
      "Epoch 8/50\n",
      "29289/29289 [==============================] - 45s 2ms/step - loss: 0.0257 - acc: 0.9907 - val_loss: 0.1730 - val_acc: 0.9459\n",
      "Epoch 9/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0242 - acc: 0.9910 - val_loss: 0.1678 - val_acc: 0.9662\n",
      "Epoch 10/50\n",
      "29184/29289 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9916\n",
      "Epoch 00010: reducing learning rate to 1.500000071246177e-05.\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0232 - acc: 0.9917 - val_loss: 0.1704 - val_acc: 0.9595\n",
      "Epoch 11/50\n",
      "29289/29289 [==============================] - 46s 2ms/step - loss: 0.0210 - acc: 0.9923 - val_loss: 0.1718 - val_acc: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd83aa742b0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([Xtrain_pl, Xinc_pl], to_categorical(Ytrain_pl),\n",
    "                                          batch_size=128, \n",
    "                                          epochs=50,\n",
    "                                          validation_data=([X_valid, X_inc_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save_weights('/home/ubuntu/data/iceberg/results/weights/norm_image_inc_angle-Copy1_model2_96_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1471/1471 [==============================] - 1s 636us/step\n",
      "Train score: 0.0680898964347\n",
      "Train accuracy: 0.982664853841\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights(filepath = '.mdl_wts.hdf5')\n",
    "score = model2.evaluate([Xtrain,Xinc], to_categorical(Ytrain), verbose=1)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "df_test = pd.read_json(os.path.join(data_dir, 'test.json'))\n",
    "df_test.inc_angle = df_test.inc_angle.replace('na',0)\n",
    "Xtest = (get_scaled_imgs(df_test))\n",
    "Xtest_inc = df_test.inc_angle\n",
    "pred_test = model.predict([Xtest,Xtest_inc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id    is_iceberg\n",
      "0  5941774d  2.231949e-03\n",
      "1  4023181e  9.414673e-01\n",
      "2  b20200e4  9.350348e-01\n",
      "3  e7f018bb  9.999995e-01\n",
      "4  4371c8c3  1.864136e-01\n",
      "5  a8d9b1fd  9.991841e-01\n",
      "6  29e7727e  3.206558e-03\n",
      "7  92a51ffb  9.999993e-01\n",
      "8  c769ac97  3.583631e-12\n",
      "9  aee0547d  8.747134e-11\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test[:,1].reshape((pred_test[:,1].shape[0]))})\n",
    "print(submission.head(10))\n",
    "\n",
    "submission.to_csv('/home/ubuntu/data/iceberg/results/final_preds/cnn_model2_pseudo_data2' + str(np.around(score[1], decimals=2))  + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8424, 75, 75, 3), (8424,), (8424, 2))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape, Xtest_inc.shape, pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25272, 75, 75, 3), (25272,), (25272, 2))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest = get_more_images(Xtest)\n",
    "Xtest_inc = np.concatenate((Xtest_inc,Xtest_inc,Xtest_inc))\n",
    "pred_test = np.concatenate((pred_test, pred_test, pred_test))\n",
    "\n",
    "Xtest.shape, Xtest_inc.shape, pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the correct way round with [:,1]\n",
    "idx_pred_1 = (np.where(pred_test[:,1]>0.95))\n",
    "idx_pred_0 = (np.where(pred_test[:,1]<0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest_inc = np.expand_dims(Xtest_inc, axis=1)\n",
    "X_inc_train = np.expand_dims(X_inc_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3969, 1), (25272, 1))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_inc_train.shape, Xtest_inc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3969,), (25272,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_inc_train = np.squeeze(X_inc_train)\n",
    "Xtest_inc = np.squeeze(Xtest_inc)\n",
    "X_inc_train.shape, Xtest_inc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22671, 75, 75, 3), (22671,), (22671, 1))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_pl = np.concatenate((X_train, Xtest[idx_pred_1[0],...], Xtest[idx_pred_0[0],...]))\n",
    "Ytrain_pl = np.concatenate((Y_train, np.ones(idx_pred_1[0].shape[0]), np.zeros(idx_pred_0[0].shape[0])))\n",
    "Xinc_pl = np.concatenate((X_inc_train, Xtest_inc[idx_pred_1[0],...], Xtest_inc[idx_pred_0[0],...]))\n",
    "\n",
    "Xtrain_pl.shape, Ytrain_pl.shape, Xinc_pl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0.3\n",
    "input_shape = (75, 75, 3)\n",
    "num_classes = 2\n",
    "\n",
    "classifier_input = Input(shape=input_shape)\n",
    "inc_angle_input = Input(shape=(1,))\n",
    "\n",
    "# CNN 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(classifier_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)   # REMOVED MAX POOLING FOR VISUALISATION\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# CNN 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "\n",
    "# CNN 4\n",
    "x = Conv2D(64,(3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "# x = BatchNormalization(axis=-1)(x)\n",
    "x = Dropout(p/2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "m = Concatenate()([inc_angle_input, x])\n",
    "m = Dense(512, activation='relu')(m)\n",
    "m = Dense(256, activation='relu')(m)\n",
    "out = Dense(2, activation='sigmoid')(m)\n",
    "# out = Activation('softmax')(m)\n",
    "\n",
    "# optimizer = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "optimizer = Adam(lr=0.0015, decay=0.0)\n",
    "model3 = Model(inputs=[classifier_input, inc_angle_input], outputs=out)\n",
    "model3.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22671 samples, validate on 148 samples\n",
      "Epoch 1/50\n",
      "22671/22671 [==============================] - 34s 2ms/step - loss: 0.5127 - acc: 0.7542 - val_loss: 0.5443 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.2778 - acc: 0.8833 - val_loss: 0.5339 - val_acc: 0.7770\n",
      "Epoch 3/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.1617 - acc: 0.9312 - val_loss: 0.3537 - val_acc: 0.8851\n",
      "Epoch 4/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.1204 - acc: 0.9495 - val_loss: 0.2666 - val_acc: 0.9054\n",
      "Epoch 5/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.1006 - acc: 0.9611 - val_loss: 0.2568 - val_acc: 0.9257\n",
      "Epoch 6/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.1026 - acc: 0.9620 - val_loss: 0.3428 - val_acc: 0.8885\n",
      "Epoch 7/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0804 - acc: 0.9690 - val_loss: 0.2730 - val_acc: 0.9155\n",
      "Epoch 8/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0809 - acc: 0.9696 - val_loss: 0.2661 - val_acc: 0.9155\n",
      "Epoch 9/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0671 - acc: 0.9755 - val_loss: 0.2898 - val_acc: 0.9054\n",
      "Epoch 10/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0716 - acc: 0.9745 - val_loss: 0.3031 - val_acc: 0.9054\n",
      "Epoch 11/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0645 - acc: 0.9757 - val_loss: 0.2708 - val_acc: 0.9155\n",
      "Epoch 12/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0628 - acc: 0.9763 - val_loss: 0.2441 - val_acc: 0.9054\n",
      "Epoch 13/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0602 - acc: 0.9777 - val_loss: 0.3357 - val_acc: 0.9155\n",
      "Epoch 14/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0532 - acc: 0.9802 - val_loss: 0.3543 - val_acc: 0.9122\n",
      "Epoch 15/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0499 - acc: 0.9812 - val_loss: 0.2431 - val_acc: 0.9257\n",
      "Epoch 16/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0521 - acc: 0.9817 - val_loss: 0.3342 - val_acc: 0.9223\n",
      "Epoch 17/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0487 - acc: 0.9816 - val_loss: 0.3455 - val_acc: 0.9324\n",
      "Epoch 18/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0467 - acc: 0.9828 - val_loss: 0.3098 - val_acc: 0.9291\n",
      "Epoch 19/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0461 - acc: 0.9834 - val_loss: 0.3114 - val_acc: 0.9189\n",
      "Epoch 20/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0425 - acc: 0.9851 - val_loss: 0.2499 - val_acc: 0.9257\n",
      "Epoch 21/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0447 - acc: 0.9842 - val_loss: 0.2737 - val_acc: 0.9088\n",
      "Epoch 22/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0520 - acc: 0.9809 - val_loss: 0.2590 - val_acc: 0.9392\n",
      "Epoch 23/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0396 - acc: 0.9854 - val_loss: 0.2963 - val_acc: 0.9257\n",
      "Epoch 24/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0416 - acc: 0.9845 - val_loss: 0.2277 - val_acc: 0.9392\n",
      "Epoch 25/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0466 - acc: 0.9829 - val_loss: 0.3114 - val_acc: 0.9426\n",
      "Epoch 26/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0412 - acc: 0.9843 - val_loss: 0.2094 - val_acc: 0.9459\n",
      "Epoch 27/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0374 - acc: 0.9869 - val_loss: 0.3088 - val_acc: 0.9189\n",
      "Epoch 28/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0378 - acc: 0.9858 - val_loss: 0.2450 - val_acc: 0.9459\n",
      "Epoch 29/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0358 - acc: 0.9874 - val_loss: 0.3099 - val_acc: 0.9392\n",
      "Epoch 30/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0383 - acc: 0.9855 - val_loss: 0.2516 - val_acc: 0.9392\n",
      "Epoch 31/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0366 - acc: 0.9868 - val_loss: 0.2829 - val_acc: 0.8885\n",
      "Epoch 32/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0333 - acc: 0.9881 - val_loss: 0.2878 - val_acc: 0.9392\n",
      "Epoch 33/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0307 - acc: 0.9888 - val_loss: 0.2901 - val_acc: 0.9459\n",
      "Epoch 34/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0301 - acc: 0.9886 - val_loss: 0.2933 - val_acc: 0.9527\n",
      "Epoch 35/50\n",
      "22528/22671 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9883\n",
      "Epoch 00035: reducing learning rate to 0.00015000000130385163.\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0320 - acc: 0.9883 - val_loss: 0.4230 - val_acc: 0.9189\n",
      "Epoch 36/50\n",
      "22671/22671 [==============================] - 34s 1ms/step - loss: 0.0209 - acc: 0.9925 - val_loss: 0.3510 - val_acc: 0.9392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe781e7860>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit([Xtrain_pl, Xinc_pl], to_categorical(Ytrain_pl),\n",
    "                                          batch_size=256, \n",
    "                                          epochs=50,\n",
    "                                          validation_data=([X_valid, X_inc_valid], to_categorical(Y_valid)),\n",
    "                                          shuffle=True, \n",
    "                                          callbacks=[earlyStopping, mcp_save, reduce_lr_loss, tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3.save_weights('/home/ubuntu/data/iceberg/results/weights/norm_image_inc_angle-Copy1_model3_data2_93_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1471/1471 [==============================] - 1s 651us/step\n",
      "Train score: 0.0615342418751\n",
      "Train accuracy: 0.986403806934\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights(filepath = '.mdl_wts.hdf5')\n",
    "score = model3.evaluate([Xtrain,Xinc], to_categorical(Ytrain), verbose=1)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "df_test = pd.read_json(os.path.join(data_dir, 'test.json'))\n",
    "df_test.inc_angle = df_test.inc_angle.replace('na',0)\n",
    "Xtest = (get_scaled_imgs(df_test))\n",
    "Xtest_inc = df_test.inc_angle\n",
    "pred_test = model.predict([Xtest,Xtest_inc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89999998"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit_nums = np.clip(pred_test[:,1], 0.1, 0.9)\n",
    "submit_nums[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id    is_iceberg\n",
      "0  5941774d  2.231949e-03\n",
      "1  4023181e  9.414673e-01\n",
      "2  b20200e4  9.350348e-01\n",
      "3  e7f018bb  9.999995e-01\n",
      "4  4371c8c3  1.864136e-01\n",
      "5  a8d9b1fd  9.991841e-01\n",
      "6  29e7727e  3.206558e-03\n",
      "7  92a51ffb  9.999993e-01\n",
      "8  c769ac97  3.583631e-12\n",
      "9  aee0547d  8.747134e-11\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test[:,1].reshape((pred_test[:,1].shape[0]))})\n",
    "print(submission.head(10))\n",
    "\n",
    "submission.to_csv('/home/ubuntu/data/iceberg/results/final_preds/cnn_model3_pseudo_aug_data2_' + str(np.around(score[1], decimals=2))  + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 4)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4413/4413 [==============================] - 3s 600us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13649047835023664, 0.95014729196998826]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([Xtrain,Xinc], to_categorical(Ytrain), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
