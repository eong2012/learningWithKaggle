{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports and base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "import os\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Conv2D, Cropping2D\n",
    "from keras.layers import MaxPooling2D, ZeroPadding2D, BatchNormalization, Activation\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n",
    "\n",
    "data_dir = '/home/ubuntu/data/iceberg'\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create generators from the pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        #rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=90,\n",
    "        horizontal_flip=True, \n",
    "        preprocessing_function=preprocess_input)\n",
    "\n",
    "test_datagen = ImageDataGenerator(#rescale=1./255, \n",
    "                                  preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/ubuntu/data/iceberg/pngs/train',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=30,\n",
    "        shuffle=False,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        '/home/ubuntu/data/iceberg/pngs/valid',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=20,\n",
    "        shuffle=False,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# precompute the convolutional layers and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vgg_base = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_precomputed_data(model, generator):\n",
    "    filenames = generator.filenames\n",
    "    conv_features = model.predict_generator(generator, (generator.n/generator.batch_size))\n",
    "    labels_onehot = to_categorical(generator.classes)\n",
    "    labels = generator.classes\n",
    "    return (filenames, conv_features, labels_onehot, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_filenames, trn_conv_features, trn_labels, trn_labels_1 = create_precomputed_data(vgg_base, train_generator)\n",
    "val_filenames, val_conv_features, val_labels, val_labels_1 = create_precomputed_data(vgg_base, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 7, 7, 512)\n",
      "(400, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "print(trn_conv_features.shape)\n",
    "print(val_conv_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert len(trn_filenames) == 1200, \"trn_filenames not as expected\"\n",
    "assert trn_conv_features.shape == (1200, 7, 7, 512), \"trn_conv_features not as expected\"\n",
    "assert trn_labels.shape == (1200, 2), \"trn_labels not as expected\"\n",
    "\n",
    "assert len(val_filenames) == 400, \"val_filenames not as expected\"\n",
    "assert val_conv_features.shape == (400, 7, 7, 512), \"val_conv_features not as expected\"\n",
    "assert val_labels.shape == (400, 2), \"val_labels not as expected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RESULTS_DIR = '/home/ubuntu/data/iceberg/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "def save_precomputed_data(filenames, conv_feats, labels, features_base_name=\"VGG16_conv_feats/trn_\"):\n",
    "    save_array(RESULTS_DIR+\"/\"+features_base_name+'filenames.dat', np.array(filenames))\n",
    "    save_array(RESULTS_DIR+\"/\"+features_base_name+'conv_feats.dat', conv_feats)\n",
    "    save_array(RESULTS_DIR+\"/\"+features_base_name+'labels.dat', np.array(labels))\n",
    "    \n",
    "save_precomputed_data(trn_filenames, trn_conv_features, trn_labels, \"VGG16_conv_feats/trn_\")\n",
    "save_precomputed_data(val_filenames, val_conv_features, val_labels, \"VGG16_conv_feats/val_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def load_precomputed_data(features_base_name=\"VGG16_conv_feats/trn_\"):\n",
    "    filenames = load_array(RESULTS_DIR+\"/\"+features_base_name+'filenames.dat').tolist()\n",
    "    conv_feats = load_array(RESULTS_DIR+\"/\"+features_base_name+'conv_feats.dat')\n",
    "    labels = load_array(RESULTS_DIR+\"/\"+features_base_name+'labels.dat')\n",
    "    return filenames, conv_feats, labels\n",
    "\n",
    "trn_filenames, trn_conv_features, trn_labels = load_precomputed_data(\"VGG16_conv_feats/trn_\")\n",
    "val_filenames, val_conv_features, val_labels = load_precomputed_data(\"VGG16_conv_feats/val_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create simple model on top of the conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier model\n",
    "\n",
    "classifier_input_shape = (7, 7, 512)\n",
    "classifier_input = Input(shape=classifier_input_shape)\n",
    "\n",
    "x= Flatten()(classifier_input)\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "                                                     \n",
    "classifier_model_v1 = Model(classifier_input, x)\n",
    "\n",
    "classifier_model_v1.compile(Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "tbCallBack = TensorBoard(log_dir='/home/ubuntu/data/iceberg/tb_logs_ice/', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/5\n",
      "1200/1200 [==============================] - 0s 241us/step - loss: 7.7613 - acc: 0.5158 - val_loss: 6.5725 - val_acc: 0.5900\n",
      "Epoch 2/5\n",
      "1200/1200 [==============================] - 0s 239us/step - loss: 7.7613 - acc: 0.5158 - val_loss: 6.5725 - val_acc: 0.5900\n",
      "Epoch 3/5\n",
      "1200/1200 [==============================] - 0s 240us/step - loss: 7.7613 - acc: 0.5158 - val_loss: 6.5725 - val_acc: 0.5900\n",
      "Epoch 4/5\n",
      "1200/1200 [==============================] - 0s 240us/step - loss: 7.7613 - acc: 0.5158 - val_loss: 6.5725 - val_acc: 0.5900\n",
      "Epoch 5/5\n",
      "1200/1200 [==============================] - 0s 234us/step - loss: 7.7613 - acc: 0.5158 - val_loss: 6.5725 - val_acc: 0.5900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3bcac15198>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model_v1.fit(trn_conv_features, trn_labels,\n",
    "                                          batch_size=32, \n",
    "                                          epochs=5,\n",
    "                                          validation_data=(val_conv_features, val_labels),\n",
    "                                          shuffle=True) #, \n",
    "#                                           callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.set_value(classifier_model_v1.optimizer.lr, 0.001)\n",
    "K.eval(classifier_model_v1.optimizer.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fully conv network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nf = 128\n",
    "p = 0. # adding any dropout at all means it doesnt train at all\n",
    "\n",
    "x = Conv2D(nf,(3,3), activation='relu', padding='same')(classifier_input)\n",
    "x = Dropout(p)(x)\n",
    "x = Conv2D(2,(3,3), padding='same')(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax')(x)\n",
    "\n",
    "classifier_model_v2 = Model(classifier_input, x)\n",
    "\n",
    "classifier_model_v2.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(classifier_model_v1.optimizer.lr, 0.001)\n",
    "K.eval(classifier_model_v1.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1200/1200 [==============================] - 0s 302us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.9275 - val_acc: 0.7725\n",
      "Epoch 2/10\n",
      "1200/1200 [==============================] - 0s 288us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 1.1152 - val_acc: 0.7350\n",
      "Epoch 3/10\n",
      "1200/1200 [==============================] - 0s 284us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 1.0190 - val_acc: 0.7650\n",
      "Epoch 4/10\n",
      "1200/1200 [==============================] - 0s 283us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 1.0358 - val_acc: 0.7775\n",
      "Epoch 5/10\n",
      "1200/1200 [==============================] - 0s 287us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 1.0824 - val_acc: 0.7575\n",
      "Epoch 6/10\n",
      "1200/1200 [==============================] - 0s 289us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 1.2013 - val_acc: 0.7375\n",
      "Epoch 7/10\n",
      "1200/1200 [==============================] - 0s 286us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 1.1072 - val_acc: 0.7600\n",
      "Epoch 8/10\n",
      "1200/1200 [==============================] - 0s 288us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 1.0968 - val_acc: 0.7750\n",
      "Epoch 9/10\n",
      "1200/1200 [==============================] - 0s 284us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 1.1022 - val_acc: 0.7725\n",
      "Epoch 10/10\n",
      "1200/1200 [==============================] - 0s 285us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 1.1599 - val_acc: 0.7675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3bc68ebda0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model_v2.fit(trn_conv_features, trn_labels, \n",
    "                                          batch_size=64, \n",
    "                                          epochs=10,\n",
    "                                          validation_data=(val_conv_features, val_labels),\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 128)         589952    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 2)           2306      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 592,258\n",
      "Trainable params: 592,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nf = 128\n",
    "p = 0. # adding any dropout at all means it doesnt train at all\n",
    "\n",
    "x = Conv2D(nf,(3,3), activation='relu', padding='same')(classifier_input)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(nf,(3,3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "# x = MaxPooling2D()(x)\n",
    "x = Conv2D(nf,(3,3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = Conv2D(nf,(3,3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "# x = MaxPooling2D()(x)\n",
    "x = Conv2D(nf,(3,3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "\n",
    "# x = MaxPooling2D((1,2))(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Conv2D(2,(3,3), padding='same')(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax')(x)\n",
    "\n",
    "classifier_model_v3 = Model(classifier_input, x)\n",
    "\n",
    "classifier_model_v3.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9999997e-06"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(classifier_model_v3.optimizer.lr, 0.00001)\n",
    "K.eval(classifier_model_v3.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1194 - acc: 0.9933 - val_loss: 0.5148 - val_acc: 0.7400\n",
      "Epoch 2/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1083 - acc: 0.9942 - val_loss: 0.5213 - val_acc: 0.7350\n",
      "Epoch 3/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.0976 - acc: 0.9967 - val_loss: 0.5166 - val_acc: 0.7325\n",
      "Epoch 4/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.0876 - acc: 0.9975 - val_loss: 0.5187 - val_acc: 0.7375\n",
      "Epoch 5/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.0795 - acc: 0.9983 - val_loss: 0.5202 - val_acc: 0.7375\n",
      "Epoch 6/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.0717 - acc: 0.9992 - val_loss: 0.5219 - val_acc: 0.7400\n",
      "Epoch 7/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.0657 - acc: 1.0000 - val_loss: 0.5230 - val_acc: 0.7425\n",
      "Epoch 8/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.0591 - acc: 0.9992 - val_loss: 0.5342 - val_acc: 0.7450\n",
      "Epoch 9/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.0530 - acc: 1.0000 - val_loss: 0.5290 - val_acc: 0.7400\n",
      "Epoch 10/10\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 0.0486 - acc: 1.0000 - val_loss: 0.5339 - val_acc: 0.7375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b9bff62e8>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model_v3.fit(trn_conv_features, trn_labels, \n",
    "                                          batch_size=64, \n",
    "                                          epochs=10,\n",
    "                                          validation_data=(val_conv_features, val_labels),\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
